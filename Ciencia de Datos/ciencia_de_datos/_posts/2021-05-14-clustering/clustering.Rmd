---
title: "Clustering"
description: |
  .
author:
  - name: Eduardo Selim M. M.
  - name: Carlos A. Ar.
date: 05-11-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
categories:
  - clustering
---

## Definiciones generales

-   Un m칠todo de clustering se utiliza para agrupar un data-set heterog칠neo en [subgrupos homog칠neos]{.ul} que se conocen como [clusters]{.ul}. Estrategia [no]{.ul} supervisada, es decir, no hay variable respuesta $y$.

-   **Revisaremos**

    -   Clustering por [k-medias]{.ul}

    -   Cluestering [jer치rquico]{.ul}

-   Se considera

    -   Si se conoce a priori el npumero de sibgrupos, se usa k-medias.

    -   Si no se conoce a priori el n칰mero de subgrupos, se usa clustering jer치rquico.

-   Un [cluster]{.ul} es un grupo de observaciones que son ["muy similares"]{.ul} y tienen varias caracter칤sticas en com칰n.

    -   Entonces se quiere [cuantificar]{.ul} la noci칩n de ["muy similar"]{.ul}

-   La cercan칤a de las observaciones est치 determinada por una [distancia.]{.ul}

    -   En principio, la distancia Euclidiana

    -   Pero pueden haber otras metricas

# Clustering por k medias

-   Se quiere agrupar al data-set como $\textbf{k}$ cl칰sters disjuntos.

-   Los datos iniciales son $n$ y est치n **enumerados** de 1 a $n$.

    -   Las etiquetas de las observaciones se guardan en $C = \{1,2,...,n\}$

-   Con clustering por k-medias se quiere determinar cl칰sters $C_1, C_2, ..., C_k$

-   Las etiquetas en $C_i$ son observaciones que pertenecen al cl칰ster $i$.

-   Cada observaci칩n ser치 asignada [a un y s칩lo un]{.ul} cl칰ster.

    -   $C = C_1 \cup C_2 \cup ... \cup C_k$

    -   Si $i \neq j$, entonces $C_i \cap C_j = \emptyset$

-   Con clustering por k-medias, se quiere encontrar cl칰sters $C_1,C_2,...,C_k$ talqes que la [variaci칩n dentro de cada cl칰ster]{.ul} ser치 lo [m치s peque침a posible.]{.ul}

-   Mientras m치s [peque침a]{.ul} sea la [variaci칩n]{.ul} within-cluster, mejor serpa el m칠todo de clustering. 游땕

-   La variaci칩n dentro de un cl칰ster se mide usando una fucni칩n $W(\cdot)$

    -   $W(C_j)$: Variaci칩n entre las observaciones en el cl칰ster $j$.

-   La variaci칩n total est치 dada por

    $$
    \text{Variaci칩n total } = \sum_{j = 1}^{k} W(C_j)
    $$

-   El clustering por k-medias determinan qu칠 cl칰sters $C_1, ..., C_k$ minimizan la variaci칩n total

    $$
    \min_{C_1, ..., C_k}\sum_{j=1}^{k}W(C_j)
    $$

-   Una forma com칰n de definir $W(\cdot)$ es como la distancia Euclidiana entre las observaciones.

-   Sup칩ngase que las observaciones $\underline{x}_i, \underline{x}_{i'}$ est치n en el k-칠simo cl칰ster

    -   Esto significa que $i, i' \in C_k$

        $$
        \text{Distancia al cuadrado entre } i \text{ y } i' \\
        =\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2
        $$

-   $W(C_k)$: Distancia al cuadrado promedio entre las observaciones en el cl칰ster $k$

    $$
    W(C_k) = \frac{1}{|C_k|}\sum_{i,i' \in C_k} \sum_{j=1}^{p}(x_{ij} - x_{i'j})^2
    $$

-   Los cl칰sters $C_1, ..., C_k$ se obtienen de tal forma que

    $$
    \min_{C_1, ..., C_k}\bigg\{ \sum_{k=1}^K \frac{1}{|C_k|}\sum_{i,i' \in C_k} \sum_{j=1}^{p}(x_{ij} - x_{i'j})^2 \bigg\}
    $$

## Algor칤tmo: cl칰stering por k-medias

1.  Paso preliminar, asignar aleatoriamente cada observaci칩n a un cl칰ster.

2.  [Iteraci칩n.]{.ul} Repetir los siguientes pasos hasta que la asignaci칩n del cl칰ster no cambie entre 2 pasos consecutivos:

    (a) Determinar el centroide $\bar{x}_k$ para cada cl칰ster $C_k$ para cada $k = 1, 2, ..., K$

    $$
    \bar{x}_k := (\bar{x}_{k1}, \bar{x}_{k2},...,\bar{x}_{kp})
    $$

    (el promedio en el cl칰ster por cada variable), donde:

    $$
    \bar{x}_{kj} = \frac{1}{|C_k|}\sum_{i \in C_k} x_{ij}, \space \space j =1,2, ...,p
    $$

    (b) Asignar a cada observaci칩n al cl칰ster del centroide m치s cercano. De hecho, una observaci칩n $\underline{x}_i$ pertenece al cl칰ster $k$.

    $$
    \sum_{j=1}^p(x_{ij}-\bar{x}_{k'j})^2 \leq \sum_{j=1}^p(x_{ij}-\bar{x}_{k'j}) \text{  para cada cl칰ster } k'
    $$

-   El algor칤tmo minimiza la distancia de las observaciones a los centroides. Sin embargo, el problema de optimizaci칩n establece minimizar la distancia entre las observaciones que pertenecen al mismo cl칰ster. La siguiente expresi칩n relaciona ambos problemas

    $$
    \frac{1}{|C_k|}\sum_{i,i'\in C_k} \sum_{j=1}^{p}(x_{ij}-x_{i췂j})^2 = 2\sum_{i \in C_k} \sum_{j=1}^{p}(x_{ij}-x_{i췂j})^2
    $$

# Cl칰stering jer치rquico

-   El m칠todo de clustering jer치rquico [no]{.ul} requiere que se especif칤que el n칰mero de cl칰sters al inicio.

-   El cl칰stering jer치rquico utiliza un dendograma

    ![](https://i.stack.imgur.com/UudJF.png){width="520"}

-   La construcci칩n del dendograma es la siguiente:

    -   Se tiene un conjunto de observaciones $\underline{x}_1,\underline{x}_2,...,\underline{x}_n$ cada una de dimensi칩n p.

    -   El dendograma se construye de abajo hacia arriba. Hasta abajo se ponen todas las observaciones de manera separada; esto indica que cada observaci칩n es su propio cl칰ster, i.e. se tienen $n$ cl칰sters.

        -   Despu칠s se "fusionan" dos cl칰sters en uno. As칤, en el segundo nivel del dendograma se tienen $n-1$ cl칰sters: $n-2$ con un elemento y 1 con dos observaciones.

        -   El siguiente paso es crear un tercer nivel fusionando de nuevo 2 cl칰sters.

        -   Se contin칰a con este proceso hasta el nivel superior en el que todas las observaciones perteneces a un cl칰ster.

    -   Hay muchas formas de hacer estas "fusiones" de dos cl칰sters en uno en cada nivel.

    -   En cada paso de la construcci칩n del dendograma se debe determinar la disimilaridad (dissimilarity)

    -   Existen diferentes medidas de disimilaridad.

    -   [Linkage:]{.ul} Disimilaridad entre 2 grupos de observaciones.

    -   Hay 4 tipos de linkage populares:

        -   Completo

        -   Individual (single)

        -   Promedio (average)

        -   Centroide

    -   **Linkage completo.** Calcula la disimilaridad entre cada punto del cl칰ster $A$ y cada punto del cl칰ster $B$. El linkage completo entre $A$ y $B$ es la distancia [m치xima]{.ul}. *Para calcular este, se hacen* $|A|\cdot|B|$ distancias y se toma la m치xima.

    -   **Linkage individual.** Calcula la disimilaridad entre cada punto del cl칰ster $A$ y cada punto del cl칰ster $B$. El linkage individual entre $A$ y $B$ es la distancia [m칤nima]{.ul}.

        -   [Observaci칩n.]{.ul} Este linkage lleva a cl칰sters "colgantes" (trailing cl칰sters), cl칰sters en los que un punto a la vez se fusionan con un single cl칰ster.

            ![](images/clustering_2.png)

    -   **Linkage promedio.** Calcula la disimilaridad entre cada punto del cl칰ster $A$ y cada punto del cl칰ster $B$. El linage promedio entre $A$ y $B$ es el [promedio de estas distancias.]{.ul}

    -   **Linkage centroide.** Calcula el centroide de cada cl칰ster y usa la disimilaridad entre los centroides.

        -   [Observaci칩n.]{.ul} Bajo este m칠todo se tiene la desventaja de que pueden ocurrir "inversores" (la disimilaridad de la fusi칩n "futura" es menor que la disimilaridad de una fusi칩n "pasada", involucrando los mismos puntos).

    -   Una vez que se contruy칩 un dendograma, se puede determinar los cl칰sters dibujando una recta horizontal en el dendograma.

        -   recta arriba $\rightarrow$ pocos cl칰sters.

        -   recta abajo $\rightarrow$ muchos cl칰sters.

-   [Within-cluster-variation]{.ul}

    $$
    W(C_k):=\sum_{i \in C_k} \sum_{j=1}^{p}(x_{ij}- \bar{x}_{kj})^2\\
    W_k := \sum_{k=1}^KW(C_k)
    $$

    -   $W_k$ debe ser peque침a

    -   $k \mapsto W_k$ es decreciente

    -   쯀ncrementa el n칰mero de cl칰sters? 游땩

-   Between-cluster-variation

    $$
    b_k = \sum_{k=1}^K|C_k|\sum_{j=1}^{p}(\bar{x}_{kj}-\bar{x}_{j})^2\\
    \text{"La suma de las distancias entre los centroides"}
    $$

    -   $b_k$ debe ser grande

    -   $k \mapsto b_k$ es creciente

    -   쯀ncrementa el n칰mero de cl칰sters? 游땩

-   칈ndice CH

    $$
    CH_k:=\frac{\frac{b_k}{k-1}}{\frac{W_k}{n-k}}
    $$

    -   Donde $n$ es el n칰mero total de observaciones en el data-set.

    -   El valor 칩ptimo para $k$ es aquel que maximiza $CH_k$
