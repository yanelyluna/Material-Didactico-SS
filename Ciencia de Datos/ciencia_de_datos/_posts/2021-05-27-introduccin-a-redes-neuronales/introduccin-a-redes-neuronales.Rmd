---
title: "Introducci贸n a redes neuronales"
description: |
  Redes neuronales 1
author:
  - name: Eduardo Selim M. M.
  - name: Carlos A. Ar.
date: 05-29-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
categories:
  - redes neuronales 
---

-   Consid茅rese un problema de clasificaci贸n binaria $G=0$ 贸 $G=1$, a partir de las covariables $x_1,...,x_p$

-   Cuando se estudi贸 regresi贸n polinomial e interacciones, b谩sicamente se consider贸 una [transformaci贸n de las variables originales existentes]{.ul}. Con esta idea en mente, se pueden construir [$m$ nuevas entradas]{.ul} de la siguiente forma:

    ![](images/rn1_1.png){width="650"}

    para $k = 1, 2, ..., m$. Donde $h(\cdot)$ una funci贸n adecuada (que se especificar谩 m谩s adelante) y por supuesto $\theta_{k0}, \theta_{k1}, ..., \theta_{kp}$ son par谩metros que eventualmente se tendr谩n que estimar.

-   Ahora se modelar谩 la probabilidad de clase 1 a partir de regresi贸n log铆stica pero en ves de usar las variables originales $x_1, ..., x_p$, se usan estas variables derivadas $a_1, ..., a_m$

    $$
    P_1(\underline{x}) = logit(\beta_0 + \beta_0 a_1 + ... + \beta_ma_m) \\
    \text{Donde a } a_1, ..., a_m \text{ se conocen como nuevas variables o variables derivadas.}
    $$

    Se puede representar esto mediante un grafo dirigido

    ![](images/rn1_2.png){width="580"}

-   La funci贸n $h(\cdot)$ debe ser [no-lineal]{.ul}; ya que si fuera lineal, en realidad no se estar铆a transformando (pues combinaciones lineales de combinaciones lineales son combinaciones lineales ).

-   Se puede demostrar que si se definen suficientes entradas derivadas (i.e. $m$ es suficientemente grande) entonces la funci贸n $P_1(\underline{x})$ puede aproximar cualquier funci贸n continua.

-   La funci贸n $h(\cdot)$ se conoce como [**funci贸n de activaci贸n.**]{.ul}

-   Algunos ejemplos de funciones de activaci贸n son;

    -   Tangente hiperb贸lica: $h(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$

    -   ReLU (Rectified Linear Unit). $h(x) = \max\{0, x\}$

    -   Sigmoide: $h(x) = \frac{1}{1+e^{-x}} = \frac{e^{x}}{e^{x}+1}$

    -   Leaky ReLU: $h(x) = \max\{\frac{x}{10}, x\}$

    -   Step (heavyside) $h(x) = 1^x_{[0, \infty)} = \begin{cases} 1, \text{ si } x\geq 0, \\ 0, \text{ si } x < 0\end{cases}$

**Tareita:** Revisar el artpiculo Cybento, G(1989). "Approximations by superpositions of sigmoidal functions". Mathematics of control, Signals and Systems 303-314. **Reporte:**

-   Este procedimiento se basa en el [Teorema de Aproximaci贸n Universal]{.ul}, que en su versi贸n simple, establece que cualquier funci贸n se puede aproximar mediante superposici贸n de funciones tipo sigmoide.

-   **Ejemplo:**

    Consid茅rese una arquitectura muy sencilla

    ![](images/rn1_3.png){width="414"}

Es decir, s贸lo se tiene una variable explicativa $x_1$ y se derivar谩n 2 nuevas variables $a_1, a_2$. Entonces, se hace una regresi贸n log铆stica para predecir $G = 1$ o $G = 0$

$$
p_1(x ; a_1, a_2) = logit(\beta_0 + \beta_1a_1 + \beta_2a_2)
$$

Donde

$$
a_1(x) := h(\beta_{10} + \beta_{11}x_1)\\
a_2(x) := h(\beta_{20} + \beta_{21}x_1) 
$$

con $h(\cdot)$ preestablecida. Para este ejemplo, sup贸ngase que $h(x) = \frac{1}{1+ e^{-x}}$

**Observaci贸n:** En esta especificaci贸n, hay 7 par谩metros, que eventualmente tendr谩n que estimar: $\beta_0, \beta_1, \beta_2, \beta_{10}, \beta_{11}, \beta_{20}, \beta_{21}$ 驴C贸mo estimar estos $\beta'^s$?

-   Como se est谩 con un problema de clasificaci贸n, se estimar谩n estos par谩metros [minimizando la devianza]{.ul} 贸 devianza regularizada

    El problema de optimizaci贸n se escribe f谩cil, pero no es sencillo de resolver.

    -   驴Existe soluci贸n?

    -   Si existe, 驴es 煤nica?

    -   驴C贸mo encuentro la soluci贸n?

        -   驴Anal铆ticamente?

        -   驴Num茅ricamente?

## Feeed forward

-   Se generalizar谩 esta idea de aproximar la respuesta para definir la arquitectura b谩sica de una red neuronal.

-   A las variables originales se les conoce como [capa de entrada]{.ul} de la red.

-   A la variable de salida se le conoce como [capa de salida.]{.ul}

-   A las capas inmediatas se les conoce como [capas ocultas.]{.ul}

-   Cuando hay todas las conexiones de una capa a la otra se dice que la red es [competamente conexa.]{.ul}

-   Como se vio en el ejemplo, para hacer los c谩lculos en la red se empezar谩 con la primera capa haciendo combinaciones lineales (operaci贸n suma) y posteriormente se aplica la funci贸n $h(\cdot)$ (funci贸n de activaci贸n).

    -   Una vez que se calcul贸 la segunda capa, se calcula la 3era de la misma forma:

        -   Combinaciones lineales (operaci贸n suma).

        -   Aplicaci贸n de $h$ (funci贸n de activaci贸n).

-   **Notaci贸n:**

    -   $L$ n煤mero total de capas

        -   1 capa de entrada

        -   1 capa de salida

        -   $L-2$ capas ocultas

    -   $x = (x_1, ..., x_p)$, variables originales.

    -   $n_l$: n煤mero de unidades (neuronas / nodos) de la capa $l$ , $l = 1, ..., L$

    -   $a_j^{(l)}$: valor que toma la unidad $j$ de la capa $l, j = 0,1, ..., n_l, l = 1, 2, ..., L$

    -   Para cualquier $l \in \{1, 2, ..., L \}, a_0^{(l)} \equiv 1$ i.e. **es** el coeficiente de sesgo/"ordenada al origen".

    -   $a_j^{(l)} \equiv x_j$ (valores de la primera capa)

    -   $\theta_{i,k}^{(l)}$: Ponderaci贸n/coeficiente dela entrada $a_k^{(l-1)}$ en la entrada $a_i^{(l)}$

**Ejemplo**

![](images/rn1_4.png){width="584"}

-   Para calcular los valores de salida de una red a partir de ponderaciones y datos de entrada, se usa el algoritmo [feed-forward]{.ul}

    -   Para la primera capa, se escriben las variables de entrada

        $$
        a_j^{(1)} := x_j, j=1,2,...,n_1, (n_1 = p)
        $$

    -   Para la segunda capa (贸 la primera capa oculta)

        $$
        a_{j}^{(2)} = h\bigg( \theta_{_{j,0}}^{^{(1)}} + \theta_{_{j,1}}^{^{(1)}}a_1^{(1)} + ... + \theta_{_{j,n_1}}^{^{(1)}}a_{n_1}^{(1)} \bigg) =h\bigg( \theta_{_{j,0}}^{^{(1)}} + \sum_{k=1}^{n_1}\theta_{_{j,k}}^{^{(1)}}a_{k}^{(1)} \bigg)\\ j = 1,2,...,n_2
        $$

    -   Para la $l$-茅sima capa

        $$
        a_{j}^{(l)} = h\bigg( \theta_{_{j,0}}^{^{(l-1)}} + \theta_{_{j,1}}^{^{(l-1)}}a_1^{(l-1)} + ... + \theta_{_{j,n_2}}^{^{(l-1)}}a_{n_2}^{(l-1)} \bigg) =h\bigg( \theta_{_{j,0}}^{^{(l-1)}} + \sum_{k=1}^{n_2}\theta_{_{j,k}}^{^{(l-1)}}a_{k}^{(l-1)} \bigg)\\ j = 1,2,...,n_l
        $$

    -   Para la capa final 贸 capa de salida (suponiendo un problema de clasificaci贸n binaria)

        $$
        p_1 = h\bigg( \theta_{_{1,0}}^{^{(L-1)}} + \sum_{k=1}^{n_{L-1}}\theta_{_{j,k}}^{^{(L-1)}}a_{k}^{(L-1)} \bigg)
        $$

        Cada capa se caracteriza por el conjunto de par谩metros

        $$
        \Theta^{(l)} \in m^{(\mathbb{R})}_{ n_l\times n_{l-1}}
        $$ i.e. $\Theta^{(l)}$ es una matriz de $n_l \times n_{l-1}$

-   La red completa se caracteriza por:

    1.  El n煤mero de capas
    2.  El n煤mero de nodos de cada capa
    3.  Las matrices de ponderaciones/coeficientes en cada capa $\Theta^{(1)}, \Theta^{(2)}, ..., \Theta^{(L-1)}$

    **Ojo:** Tambi茅n se podr铆a variar la funci贸n de activaci贸n en cada capa.

**Notaci贸n:** $a^{(l)} := (a_0^{(l)}, a_1^{(l)}, ..., a_{n_l}^{(l)})$ i.e. el vector de unidades de la capa $l$.

Entonces el algoritmo feed-forward en notaci贸n matricial es:

-   Capa 1: $a^{(1)} = x \in \mathbb{R}^p, i.e., n_1 = p$

-   Capa: 2: $a^{(2)} = h(\underbrace{\Theta^{(1)}}_{n_2 \times p} \underbrace{a^{(1)}}_{p\times1}) \in \mathbb{R}^{n_2}$

    -   **隆Ojo!:** Abuso de notaci贸n. $h$ se aplica componente a componente sobre los vectores correspondientes.

-   Capa $l$ (capa oculta)

    $$
    a^{(l)} = h(\underbrace{\Theta^{(1)}}_{n_l \times n_{l-1}} \space \space \space \underbrace{a^{(1)}}_{n_{l-1}\times1}) \in \mathbb{R}^{n_l}
    $$

-   Capa de salida:

    -   Para un problema de clasificaci贸n binaria

        $$
        a^{(L)} = p_1 = logit(\Theta^{(L-1)} a^{(L-1)})
        $$

    -   Para un problema de regresi贸n

        $$
        a^{(L)} = \Theta^{(L-1)} a^{(L-1)}
        $$
