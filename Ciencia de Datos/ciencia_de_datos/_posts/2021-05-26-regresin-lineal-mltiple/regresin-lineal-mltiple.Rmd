---
title: "Regresi칩n lineal m칰ltiple"
description: |
  Repaso de regresi칩n lineal con m칰ltiples variables explicativas.
author:
  - name: Eduardo Selim M. M.
  - name: Carlos A. Ar.
date: 05-26-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
categories:
  - regresion
---

$$
y = \underbrace{\beta_0 + \beta_1x_1 + \beta_2x_2 +...+ \beta_kx_k+\epsilon}_{\text{funci칩n de regresi칩n}}
$$

Muestralmente

$$
(y_i, \underbrace{x_{i1},x_{i1},...,x_{ik}}_{\text{Covariables, } \\ \text{variables predictoras,} \\ \text{features, } \\ \text{variables explicativas}}), \space \space \space \space i = 1,2,...,n \\ \text{Donde } n \text{ es el n칰mero de observaciones.} 
$$

Entonces $y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + \epsilon_i$ donde $\epsilon_i \sim N(0 , 1)$ adem치s de ser i.i.d.

Como antes

$$
\mathbb{E}(y_i)= \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik}
$$

Interpretaci칩n muy popular es que $\beta_j$ es el cambio esperado en $y$, por unidad de cambio en $x_j$ *(ceteris paribus)* puesto que

$$
\frac{\partial \mathbb{E}(y_i)}{\partial x_j} = \beta_j
$$

En t칠rminos matriciales

$$
\begin{equation}
 \underbrace{\begin{pmatrix}
   y_1 \\
   y_2 \\
   \vdots \\
   y_n
 \end{pmatrix}}_{y_{n \times 1}}
 = 
\underbrace{\begin{pmatrix}
   1     & x_{11} & x_{12} & \dotso & x_{1k}\\
   1     & x_{21} & x_{22} & \dotso & x_{2k}\\
  \vdots & \vdots & \vdots & \ddots & \vdots\\
   1     & x_{n1} & x_{n2} & \dotso & x_{nk}
 \end{pmatrix}}_{\mathbb{X}_{n \times (k+1)} \\ \text{Matr칤z de covariables} \\ \text{Matriz de dise침o}}
\underbrace{\begin{pmatrix}
   \beta_0 \\
   \beta_1 \\
   \vdots \\
   \beta_k
 \end{pmatrix}}_{\beta_{(k+1) \times 1}}
+
\underbrace{\begin{pmatrix}
   \epsilon_1 \\
   \epsilon_2 \\
   \vdots \\
   \epsilon_n
 \end{pmatrix}}_{\epsilon_{n \times 1}}
\end{equation}
$$

Nos hacemos las mismas preguntas de siempre

$$
\rightarrow 쯒hat{\beta}? \text{ 쮺칩mo obtengo los estimadores?}\\
$$

$$
\rightarrow y_i \sim \hat{y}_i, \space \space \space \space \space  \space \space \space \space \space \space \space \Rightarrow \space \space \space \space  \space\space \space \space \space  \space \space \space  e_i = y_i - \hat{y}_i\\
$$

$$
\rightarrow \text{La certidumbre tanto de } \hat{\beta} \text{ como de } \hat{y}\\ \text{(i.e. intervalos de confianza)} \\
$$

$$
\rightarrow \text{Predicci칩n: 쮺칩mo se comporta el modelo ante variables explicativas no observadas?}\\
$$

$$
\rightarrow \text{Future Engineering, selecci칩n de variables} \\
 \text{쯈u칠 variables aportan a explicar } y \text{?}
$$

## Estimador de $\beta$

Se obtiene por m칤nimos cuadrados

-   Se obtienen lo que se conoce como las ecuaciones normales.

    $$
    y = \mathbb{X} \beta \underbrace{\Rightarrow }_{\text{Multiplicamos por }  \mathbb{X}^T} \mathbb{X}^T y = \mathbb{X}^T \mathbb{X} \beta \\
    \underbrace{\Rightarrow }_{\text{Estamos suponiendo que }\\ \text{esta matr칤z es invertible}} \hat{\beta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T y
    $$

Como antes

$$
RSS = \displaystyle \sum_{i=1}^n (y_i - \hat{y})^2  \\
RegSS = \displaystyle \sum_{i=1}^n(\hat{y}_i - \bar{y})^2 \\
 \text{ Estamos comparando el modelo de regresi칩n}\\
\displaystyle TSS = \sum_{i=1}^n (y_i - \bar{y})^2 = (n-1) S_y^2\\
\text{Versus el modelo naiive}\\
\text{Valor } F := \frac{RegSS/k}{Rss/(n-(k+1))}\\
\text{Se utiliza para evaluar si las } k \text{ variables explicativas}\\
\text{son colectivamente 칰tiles para explicar.}
$$

-   Con la hip칩tesis de normalidad se demuestra que:

    $$
    \hat{\beta} \sim N_{k+1} \bigg(\beta, \sigma^2(\mathbb{X}^T\mathbb{X})^{-1}\bigg)
    $$

-   **Definici칩n.** (Coeficiente de determinaci칩n $R^2$)

    $$
    R^2 = \frac{RegSS}{TSS} = \frac{\displaystyle \sum_{i=1}^n(\hat{y}_i - \bar{y})^2}{\displaystyle \sum_{i=1}^n(y_i - \bar{y})^2}
    $$

    Tristemente 游땩en MLR (Multiple Linear Regression)**ya no se cumple que**

    $$
    R^2 = r^2
    $$

    Sin embargo, s칤 se cumple que

    $$
    R^2 = \bigg[\frac{\displaystyle \sum_{i=1}^n(y_i - \bar{y})(\hat{y}_i - \bar{y})}{\underbrace{\sqrt{\displaystyle \sum_{i=1}^n(y_i - \bar{y})^2\sum_{i=1}^n(\hat{y}_i - \bar{y})^2}}_{\text{Es el cuadrado de la correlaci칩n} \\ \text{muestral entre }y \text{ y } \hat{y}}}\bigg]^2
    $$

$$
F = \frac{n-k-1}{k} \cdot \frac{R^2}{1-R^2}
$$

Ahora s칤 a construir el intervalo de confianza.

### Intervalo de confianza para $\hat{\beta}$

Est치 dado por

$$
\hat{\beta}_j \pm t_{n-(k+1), \frac{\alpha}{2}} \sqrt{S^2 \bigg(\mathbb{X}^T\mathbb{X}\bigg)^{-1}_{j-1, j+1}}
$$
