---
title: "Selección de variables"
description: |
  Si tenemos muchas variables explicativas, hay que saber con cuáles quedarnos.
author:
  - name: Eduardo Selim M. M.
  - name: Carlos A. Ar.
date: 06-06-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
categories:
  - regresion
---

## Selección de variables

Para seleccionar un modelo de otro, las métricas siguientes nos ayudan a comparar entre modelos:

-   BIC

-   AIC

-   $R^2$

-   $R^2_a: R^2$ ajustada

-   pseudo-$R^2$

-   Estadística $C_p$ de Mallow

Estas métricas sirven para "comparar" dos modelos. Uso "comparar" entre comillas porque a veces la comparación [no]{.ul} es justa. El caso de $R^2$ es un ejemplo claro porque:

-   Más variables explicativa $\rightarrow R^2$ más grande.

-   Si nos vamos con la trampa de que $R^2$ grande es "mejor" modelo, siempre tendremos modelos con **muchas** variables explicativas y eso no necesariamente significa mejor modelo.

    -   Poca parsimonia (muy complicado).

    -   Unas variables le estorban a otras.

### $R^2_a: R^2$ ajustada

Definimos esta métrica que **sí considera** el efecto de agregar variables explicativas, como sigue:

$$
R_a^2 := 1-\frac{\frac{RSS}{n-k-1}}{\frac{TSS}{n-1}} \\
n: \text{ número de observaciones} \\
k: \text{ número de variables explicativas en el modelo}
$$

**Observaciones**

-   $R_a^2 = 1- \frac{S^2}{S^2_y}$ donde $S^2$ es el error cuadrático medio (MSE) del modelo de regresión en cuestión.

-   $R_a^2 \neq \frac{\frac{RegSS}{k}}{\frac{TSS}{n-1}}$

-   Se puede demostrar que $R_a^2 = 1- \frac{n-1}{n-k-1} (1-R^2)$ lo cual implica

    -   $R_a^2 \leq R^2 \leq 1$

    -   $R_a^2$ puede ser negativa. Esto puede ocurrir cuando $R^2$ es muy bajo (por ejemplo $R^2 \approx 0$)

        $$
        R_a^2 \approx 1-\frac{n-1}{n-k-1} = \frac{-k}{n-k-1} < 0
        $$

-   Como criterio para releccionar modelo nos sigue interesando $R_a^2$ grande.

-   Se dice que $R_a^2$ penaliza la inclusión de predicciones extrañas al modelo.

-   $R_a^2$ más alta es equivalmente a MSE chico (i.e. $S^2$ chico)

### Estadística $C_p$ de Mallow

$$
C_p := 
\begin{cases}
\frac{(RSS)_p + 2p\cdot S^2_{full}}{n}, \text{ James, Tibihrani, et al} \\
\frac{(RSS)_p}{S^2_{full}} - n + 2p, \text{ Frees.}
\end{cases}
$$

donde

-   $S^2_{full}$ es el MSE que tiene [**todas**]{.ul} las variables explicativas, no necesariamente el modelo en consideración.

-   $(RSS)_p$: Es el $RSS$ de el modelo bajo consideración, con $p$ variables explicativas.

Se puede demostrar (de forma algebráica)

$$
C_p^{\text{James}} = \frac{S^2_{full}}{n} (C_p^{\text{Frees}} + n)
$$

-   Es deseable un valor pequeño de $C_p$

-   Mientras más variables tenga el modelo, más grande será la penalización sobre $C_p$

### AIC & BIC (para modelos lineales)

```{=tex}
\begin{align*}
\text{AIC} &= \frac{(RSS)_p + 2p \cdot S^2_{full}}{n \cdot S^2_{full}} \\
\text{BIC} &= \frac{(RSS)_p + \log(n) \cdot p \cdot S^2_{full}}{n \cdot S^2_{full}} 
\end{align*}
```
¿Cuándo $\log(n)$ es mayor que 2?

$$
\log(n) > 2 \iff n> e^2 = 7.38 \approx 8
$$

Para un modelo fijo podemos saber que BIC$>$AIC cuando $n>8$, lo cual es muy común.

-   Nótese que $\text{AIC} = \frac{C_p^{\text{James}}}{S_{full}^2}$

-   Queremos AIC y BIC chicos.

## Mejor selección de subconjunto

-   Dadas $k$ variables explicativas potenciales, nos gustaría encontrar el "mejor" subconjunto de éstas para considerarlas en el modelo de regresión. El "mejor" es en el sentido de que tengan $\underbrace{AIC\text{ ó }BIC\text{ ó }C_p}_{\text{chicos}}$ ó $\underbrace{R_a^2}_{\text{grande}}$

-   ¿Cuántos posibles subconjuntos tenemos?

    $$
    2^k \text{ Con intercepto}
    $$

    Aunque es conceptualmente sencillo de entender qué es la selección de variables, es [computacionalmente más costoso]{.ul}.

-   **Ejemplo**

    $$
    y \sim x_1 + x_2 + x_3
    $$

    ¿Cuántos posibles modelos tenemos? $8 = 2^3$

    1.  $y = \beta_0$

    2.  $y = \beta_0 + \beta_1x_1$

    3.  $y = \beta_0 + \beta_2x_2$

    4.  $y = \beta_0 + \beta_3x_3$

    5.  $y = \beta_0 + \beta_1x_1 + \beta_2x_2$

    6.  $y = \beta_0 + \beta_1x_1 + \beta_3x_3$

    7.  $y = \beta_0 + \beta_2x_2 + \beta_3x_3$

    8.  $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3$

    -   Si ocupamos el $C_p$ como métrica, comparamos $C^{[1]}, C^{[2]}, ..., C^{[8]}$ y nos quedamos con el modelo que tenga el $C_p$ más pequeño.

-   Si tenemos 10 variables explicativas, tendríamos $2^{10} = 1024$ modelos a comparar, es costoso computacionalmente.

-   Dada esta complejidad, tenemos 3 alternativas que buscan eficiencia computacional pero no encuentran necesariamente el mejor modelo.

    -   [Selección forward]

        -   [Con regla de paro]

        -   Sin regla de paro

    -   Selección backward

    -   Selección stepwise

### Selección forward

-   Se empieza con el modelo más simple, i.e. el iid, i.e. $\underbrace{y = \beta_0}_{\text{se conoce como} \\ \text{el modelo nulo}}$

-   Se van agregando variables explicativas si es que está "justificado". ¿Qué significa que esté "justificado"?

    -   Si agregar la variable (en el modelo) mejora la métrica de evaluación ($AIC, BIC, C_p, R_a^2, pseudoR^2$)

#### Con regla de paro

Empezamos con $y = \beta_0 + \epsilon$

-   **Paso F1:** De entre $x_1, x_2, ..., x_k$, agregas la variable más significativa al modelo nulo. i.e. se ajustan $k$ modelos:

    \begin{align*}
    &\text{V1: }y = \beta_0 + \beta_1 x_1 + \epsilon \\
    &\text{V2: }y = \beta_0 + \beta_2 x_2 + \epsilon \\
    & \space \space \space \space \space \vdots \\
    & \text{Vk: }y = \beta_0 + \beta_k x_k + \epsilon
    \end{align*}

    Y se selecciona la variable explicativa que mejora alguna métrica seleccionada.

    Acá podría ocupar como métrica incluso la $R^2$ pues todos los modelos tiene el mismo número de variables explicativas.

-   **Paso F2:** Fijar las variables explicativas identificada en el paso anterior y buscar de entre las variables seleccionadas, aquella que sea la más estadísticamente significativa (i.e. la que tenga un $t-ratio$ mejor, $R^2$ más grande, ó un mejor $BIC/AIC/C_p/ R_a^2$)

-   **Paso F3:** Se repiten los pasos **F1** y **F2** para las variables explicativas restantes hasta que agregar más variables no sea suficientemente significativo, de acuerdo a alguna regla de paro pre-especificada (por ejemplo $t-ratio$ sea menor que $2$ (en valor absoluto) ó $R^2>85\%$, ó alguna otra arbitraria).

Si llegamos hasta agregar las $k$ variables ¿Cuántos modelos ajustamos?

$$
\underbrace{1}_{\text{modelo nulo}} + [\underbrace{k}_{\text{ajustes con }\\ 1\text{ variable}} + (k-1) + ... + \underbrace{1}_{\text{ajuste con}\\ \text{las }k \text{ variables}}] = 1+ \frac{k(k+1)}{2} \leq 2^k
$$

Lo cual implica que con este algoritmo, reducimos el número de comparaciones. El problema es que no logramos hacer todas las comparaciones, solo decimos: encontramos el mejor modelo, suponiendo que debemos dejar una variable (correspondiente al paso F2 del algoritmo).

En resumen la ventaja es mayor eficiencia pero la desventaja es que no encontramos el "mejor modelo global" (lo descrito al inicio de esta nota).
