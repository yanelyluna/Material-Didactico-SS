---
title: "Selección de variables"
description: |
  Si tenemos muchas variables explicativas, hay que saber con cuáles quedarnos.
author:
  - name: Eduardo Selim M. M.
  - name: Carlos A. Ar.
date: 06-06-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
categories:
  - regresion
---

## Selección de variables

Para seleccionar un modelo de otro, las métricas siguientes nos ayudan a comparar entre modelos:

-   BIC

-   AIC

-   $R^2$

-   $R^2_a: R^2$ ajustada

-   pseudo-$R^2$

-   Estadística $C_p$ de Mallow

Estas métricas sirven para "comparar" dos modelos. Uso "comparar" entre comillas porque a veces la comparación [no]{.ul} es justa. El caso de $R^2$ es un ejemplo claro porque:

-   Más variables explicativa $\rightarrow R^2$ más grande.

-   Si nos vamos con la trampa de que $R^2$ grande es "mejor" modelo, siempre tendremos modelos con **muchas** variables explicativas y eso no necesariamente significa mejor modelo.

    -   Poca parsimonia (muy complicado).

    -   Unas variables le estorban a otras.

### $R^2_a: R^2$ ajustada

Definimos esta métrica que **sí considera** el efecto de agregar variables explicativas, como sigue:

$$
R_a^2 := 1-\frac{\frac{RSS}{n-k-1}}{\frac{TSS}{n-1}} \\
n: \text{ número de observaciones} \\
k: \text{ número de variables explicativas en el modelo}
$$

**Observaciones**

-   $R_a^2 = 1- \frac{S^2}{S^2_y}$ donde $S^2$ es el error cuadrático medio (MSE) del modelo de regresión en cuestión.

-   $R_a^2 \neq \frac{\frac{RegSS}{k}}{\frac{TSS}{n-1}}$

-   Se puede demostrar que $R_a^2 = 1- \frac{n-1}{n-k-1} (1-R^2)$ lo cual implica

    -   $R_a^2 \leq R^2 \leq 1$

    -   $R_a^2$ puede ser negativa. Esto puede ocurrir cuando $R^2$ es muy bajo (por ejemplo $R^2 \approx 0$)

        $$
        R_a^2 \approx 1-\frac{n-1}{n-k-1} = \frac{-k}{n-k-1} < 0
        $$

-   Como criterio para releccionar modelo nos sigue interesando $R_a^2$ grande.

-   Se dice que $R_a^2$ penaliza la inclusión de predicciones extrañas al modelo.

-   $R_a^2$ más alta es equivalmente a MSE chico (i.e. $S^2$ chico)

### Estadística $C_p$ de Mallow

$$
C_p := 
\begin{cases}
\frac{(RSS)_p + 2p\cdot S^2_{full}}{n}, \text{ James, Tibihrani, et al} \\
\frac{(RSS)_p}{S^2_{full}} - n + 2p, \text{ Frees.}
\end{cases}
$$

donde

-   $S^2_{full}$ es el MSE que tiene [**todas**]{.ul} las variables explicativas, no necesariamente el modelo en consideración.

-   $(RSS)_p$: Es el $RSS$ de el modelo bajo consideración, con $p$ variables explicativas.

Se puede demostrar (de forma algebráica)

$$
C_p^{\text{James}} = \frac{S^2_{full}}{n} (C_p^{\text{Frees}} + n)
$$

-   Es deseable un valor pequeño de $C_p$

-   Mientras más variables tenga el modelo, más grande será la penalización sobre $C_p$

### AIC & BIC (para modelos lineales)

```{=tex}
\begin{align*}
\text{AIC} &= \frac{(RSS)_p + 2p \cdot S^2_{full}}{n \cdot S^2_{full}} \\
\text{BIC} &= \frac{(RSS)_p + \log(n) \cdot p \cdot S^2_{full}}{n \cdot S^2_{full}} 
\end{align*}
```
¿Cuándo $\log(n)$ es mayor que 2?

$$
\log(n) > 2 \iff n> e^2 = 7.38 \approx 8
$$

Para un modelo fijo podemos saber que BIC$>$AIC cuando $n>8$, lo cual es muy común.

-   Nótese que $\text{AIC} = \frac{C_p^{\text{James}}}{S_{full}^2}$

-   Queremos AIC y BIC chicos.

## Mejor selección de subconjunto

-   Dadas $k$ variables explicativas potenciales, nos gustaría encontrar el "mejor" subconjunto de éstas para considerarlas en el modelo de regresión. El "mejor" es en el sentido de que tengan $\underbrace{AIC\text{ ó }BIC\text{ ó }C_p}_{\text{chicos}}$ ó $\underbrace{R_a^2}_{\text{grande}}$

-   ¿Cuántos posibles subconjuntos tenemos?

    $$
    2^k \text{ Con intercepto}
    $$

    Aunque es conceptualmente sencillo de entender qué es la selección de variables, es [computacionalmente más costoso]{.ul}.

-   **Ejemplo**

    $$
    y \sim x_1 + x_2 + x_3
    $$

    ¿Cuántos posibles modelos tenemos? $8 = 2^3$

    1.  $y = \beta_0$

    2.  $y = \beta_0 + \beta_1x_1$

    3.  $y = \beta_0 + \beta_2x_2$

    4.  $y = \beta_0 + \beta_3x_3$

    5.  $y = \beta_0 + \beta_1x_1 + \beta_2x_2$

    6.  $y = \beta_0 + \beta_1x_1 + \beta_3x_3$

    7.  $y = \beta_0 + \beta_2x_2 + \beta_3x_3$

    8.  $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3$

    -   Si ocupamos el $C_p$ como métrica, comparamos $C^{[1]}, C^{[2]}, ..., C^{[8]}$ y nos quedamos con el modelo que tenga el $C_p$ más pequeño.

-   Si tenemos 10 variables explicativas, tendríamos $2^{10} = 1024$ modelos a comparar, es costoso computacionalmente.

-   Dada esta complejidad, tenemos 3 alternativas que buscan eficiencia computacional pero no encuentran necesariamente el mejor modelo.

    -   [Selección forward]

        -   [Con regla de paro]

        -   Sin regla de paro

    -   Selección backward

    -   Selección stepwise

### Selección forward

-   Se empieza con el modelo más simple, i.e. el iid, i.e. $\underbrace{y = \beta_0}_{\text{se conoce como} \\ \text{el modelo nulo}}$

-   Se van agregando variables explicativas si es que está "justificado". ¿Qué significa que esté "justificado"?

    -   Si agregar la variable (en el modelo) mejora la métrica de evaluación ($AIC, BIC, C_p, R_a^2, pseudoR^2$)

#### Con regla de paro

Empezamos con $y = \beta_0 + \epsilon$

-   **Paso F1:** De entre $x_1, x_2, ..., x_k$, agregas la variable más significativa al modelo nulo. i.e. se ajustan $k$ modelos:

    ```{=tex}
    \begin{align*}
    &\text{V1: }y = \beta_0 + \beta_1 x_1 + \epsilon \\
    &\text{V2: }y = \beta_0 + \beta_2 x_2 + \epsilon \\
    & \space \space \space \space \space \vdots \\
    & \text{Vk: }y = \beta_0 + \beta_k x_k + \epsilon
    \end{align*}
    ```
    Y se selecciona la variable explicativa que mejora alguna métrica seleccionada.

    Acá podría ocupar como métrica incluso la $R^2$ pues todos los modelos tiene el mismo número de variables explicativas.

-   **Paso F2:** Fijar las variables explicativas identificada en el paso anterior y buscar de entre las variables seleccionadas, aquella que sea la más estadísticamente significativa (i.e. la que tenga un $t-ratio$ mejor, $R^2$ más grande, ó un mejor $BIC/AIC/C_p/ R_a^2$)

-   **Paso F3:** Se repiten los pasos **F1** y **F2** para las variables explicativas restantes hasta que agregar más variables no sea suficientemente significativo, de acuerdo a alguna regla de paro pre-especificada (por ejemplo $t-ratio$ sea menor que $2$ (en valor absoluto) ó $R^2>85\%$, ó alguna otra arbitraria).

Si llegamos hasta agregar las $k$ variables ¿Cuántos modelos ajustamos?

$$
\underbrace{1}_{\text{modelo nulo}} + [\underbrace{k}_{\text{ajustes con }\\ 1\text{ variable}} + (k-1) + ... + \underbrace{1}_{\text{ajuste con}\\ \text{las }k \text{ variables}}] = 1+ \frac{k(k+1)}{2} \leq 2^k
$$

Lo cual implica que con este algoritmo, reducimos el número de comparaciones. El problema es que no logramos hacer todas las comparaciones, solo decimos: encontramos el mejor modelo, suponiendo que debemos dejar una variable (correspondiente al paso F2 del algoritmo).

En resumen la ventaja es mayor eficiencia pero la desventaja es que no encontramos el "mejor modelo global" (lo descrito al inicio de esta nota).

### Hacia atrás (selección backward)

Dadas $x_1, x_2, .., x_k$ variables explactivas potenciales, $x_k$.

Se empieza con el modelo más "completo" i.e. el de $k$ variables.

$$
\mu_k: y = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k + \epsilon 
$$

Y se eliminan las variables estadísticamente menos significativas en cada paso.

¿Qué significa una vairbale "menos significativa"?

Una variable es la **más insignificante** si satisface alguna de estas condiciones (que son equivalentes):

1.  Si $t-rato$ asociado es el **más pequeño** (en valor absoluto), de entre todas las variables explicativas presentes.

2.  El RSS del modelo resultante es el más pequeño, i.e. borrar la variable del mismo modelo con el menor incremento en el RSS y la menor cantidad de importancia sobre el modelo.

    -   ¿Por qué se vale? Como se están eliminando predictores, el RSS debe incrementarse ($y_i$ se aleja de $\hat{y}_i$ que "trae" menos info), [pero]{.ul} se busca la variable con el menor incremento.

    -   El $R^2$ resultante sea el más alto.

-   Como en el caso de la selección forward, debe haber una regla de paro; por ejemplo, una muy común es

    -   Detener el proceso de eliminación cuando todos los predictores conjuntamente en el modelo son suficientemente significativo (por ejemplo vía la **prueba F**).

    -   Otra alternativa, es terminar hasta el modelo nulo ($\mu_0$) y seleccionar el "mejor" modelo de entre $\mu_0, \mu_1, ..., \mu_k$ basándose en $AIC/BIC/C_p$, i.e. se compraron $k+1$ modelos con algún criterio $AIC/BIC/C_p$.

**Tarea:** hacer una presentación sobre la forma que se implementa la selección de variables en las librerías `Broom`, `Recipies` y `Tidymodels`.

**Importante:**

-   Si $n\leq k$, selección backward [no]{.ul} se puede implementar, ya que el modelo completo no se puede ajustar y el proceso no puede empezar.

-   La selección forward, en [principio]{.ul} no tiene problema, pues

    -   Si $n\geq k$, siempre jala.

    -   Si $n<k$

        \begin{align*}
        y &\sim \beta_0 \space \checkmark \\
        y &\sim \beta_0 + \beta_1 \space \checkmark \\
        & \vdots \\
        y &\sim \beta_0 + \beta_1 + ... + \beta_n \space \checkmark \\
        y &\sim \beta_0 + \beta_1 + ... + \beta_n + \beta_{n+1} \space \times 
        \end{align*}

### Selección stepwise

Es un híbrido entre backward y forward. Funciona como en selección forward pero con una regla de paro, pues se re-evalúa la significancia estadística de todas las variables en el modelo en cuestión.

-   Aquellos que no aporten aporten suficiente poder explicativo, se eliminan.

Bajo este procedimiento una variable explicativa puede entrar ó salir del modelo durante la ejecución del algoritmo de selección.

#### Ejemplo:

$$
\begin{align*}
&x_1, x_2, ..., x_{10}\\
1. &y \sim 1+ x_1 \\
 &\boxed{y \sim 1+ x_2} \\
 &\vdots \\
& y\sim 1+ x_{10} \\

2.  &\underline{y \sim 1+ x_2} \\
&\downarrow \\
3. & y \sim 1+ x_2 + x_1 \\
& y \sim 1+x_2 + x_3 \\
& y\sim 1+ x_2 + x_3 \\
&\vdots \\
&y \sim 1+ x_2 + x_{10} 
\end{align*}
$$

[Con forward]{.ul} se hacen una pregunta adicional ¿$x_2$ y $x_4$ son significativas?

-   Si sí $\rightarrow$ continúan agregando.

-   Si no $rightarrow$ sacan las que no son significativas y continuan agregando.
