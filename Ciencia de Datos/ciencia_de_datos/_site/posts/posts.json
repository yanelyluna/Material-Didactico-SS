[
  {
    "path": "posts/2021-05-26-regresin-lineal-mltiple/",
    "title": "Regresi√≥n lineal m√∫ltiple",
    "description": "Repaso de regresi√≥n lineal con m√∫ltiples variables explicativas.",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "regresion"
    ],
    "contents": "\r\n\r\nContents\r\nEstimador de \\(\\beta\\)\r\nIntervalo de confianza para \\(\\hat{\\beta}\\)\r\n\r\n\r\n\\[\r\ny = \\underbrace{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +...+ \\beta_kx_k+\\epsilon}_{\\text{funci√≥n de regresi√≥n}}\r\n\\]\r\nMuestralmente\r\n\\[\r\n(y_i, \\underbrace{x_{i1},x_{i1},...,x_{ik}}_{\\text{Covariables, } \\\\ \\text{variables predictoras,} \\\\ \\text{features, } \\\\ \\text{variables explicativas}}), \\space \\space \\space \\space i = 1,2,...,n \\\\ \\text{Donde } n \\text{ es el n√∫mero de observaciones.} \r\n\\]\r\nEntonces \\(y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_k x_{ik} + \\epsilon_i\\) donde \\(\\epsilon_i \\sim N(0 , 1)\\) adem√°s de ser i.i.d.\r\nComo antes\r\n\\[\r\n\\mathbb{E}(y_i)= \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_k x_{ik}\r\n\\]\r\nInterpretaci√≥n muy popular es que \\(\\beta_j\\) es el cambio esperado en \\(y\\), por unidad de cambio en \\(x_j\\) (ceteris paribus) puesto que\r\n\\[\r\n\\frac{\\partial \\mathbb{E}(y_i)}{\\partial x_j} = \\beta_j\r\n\\]\r\nEn t√©rminos matriciales\r\n\\[\r\n\\begin{equation}\r\n \\underbrace{\\begin{pmatrix}\r\n   y_1 \\\\\r\n   y_2 \\\\\r\n   \\vdots \\\\\r\n   y_n\r\n \\end{pmatrix}}_{y_{n \\times 1}}\r\n = \r\n\\underbrace{\\begin{pmatrix}\r\n   1     & x_{11} & x_{12} & \\dotso & x_{1k}\\\\\r\n   1     & x_{21} & x_{22} & \\dotso & x_{2k}\\\\\r\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\r\n   1     & x_{n1} & x_{n2} & \\dotso & x_{nk}\r\n \\end{pmatrix}}_{\\mathbb{X}_{n \\times (k+1)} \\\\ \\text{Matr√≠z de covariables} \\\\ \\text{Matriz de dise√±o}}\r\n\\underbrace{\\begin{pmatrix}\r\n   \\beta_0 \\\\\r\n   \\beta_1 \\\\\r\n   \\vdots \\\\\r\n   \\beta_k\r\n \\end{pmatrix}}_{\\beta_{(k+1) \\times 1}}\r\n+\r\n\\underbrace{\\begin{pmatrix}\r\n   \\epsilon_1 \\\\\r\n   \\epsilon_2 \\\\\r\n   \\vdots \\\\\r\n   \\epsilon_n\r\n \\end{pmatrix}}_{\\epsilon_{n \\times 1}}\r\n\\end{equation}\r\n\\]\r\nNos hacemos las mismas preguntas de siempre\r\n\\[\r\n\\rightarrow ¬ø\\hat{\\beta}? \\text{ ¬øC√≥mo obtengo los estimadores?}\\\\\r\n\\]\r\n\\[\r\n\\rightarrow y_i \\sim \\hat{y}_i, \\space \\space \\space \\space \\space  \\space \\space \\space \\space \\space \\space \\space \\Rightarrow \\space \\space \\space \\space  \\space\\space \\space \\space \\space  \\space \\space \\space  e_i = y_i - \\hat{y}_i\\\\\r\n\\]\r\n\\[\r\n\\rightarrow \\text{La certidumbre tanto de } \\hat{\\beta} \\text{ como de } \\hat{y}\\\\ \\text{(i.e. intervalos de confianza)} \\\\\r\n\\]\r\n\\[\r\n\\rightarrow \\text{Predicci√≥n: ¬øC√≥mo se comporta el modelo ante variables explicativas no observadas?}\\\\\r\n\\]\r\n\\[\r\n\\rightarrow \\text{Future Engineering, selecci√≥n de variables} \\\\\r\n \\text{¬øQu√© variables aportan a explicar } y \\text{?}\r\n\\]\r\nEstimador de \\(\\beta\\)\r\nSe obtiene por m√≠nimos cuadrados\r\nSe obtienen lo que se conoce como las ecuaciones normales.\r\n\\[\r\ny = \\mathbb{X} \\beta \\underbrace{\\Rightarrow }_{\\text{Multiplicamos por }  \\mathbb{X}^T} \\mathbb{X}^T y = \\mathbb{X}^T \\mathbb{X} \\beta \\\\\r\n\\underbrace{\\Rightarrow }_{\\text{Estamos suponiendo que }\\\\ \\text{esta matr√≠z es invertible}} \\hat{\\beta} = (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^T y\r\n\\]\r\nComo antes\r\n\\[\r\nRSS = \\displaystyle \\sum_{i=1}^n (y_i - \\hat{y})^2  \\\\\r\nRegSS = \\displaystyle \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 \\\\\r\n \\text{ Estamos comparando el modelo de regresi√≥n}\\\\\r\n\\displaystyle TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1) S_y^2\\\\\r\n\\text{Versus el modelo naiive}\\\\\r\n\\text{Valor } F := \\frac{RegSS/k}{Rss/(n-(k+1))}\\\\\r\n\\text{Se utiliza para evaluar si las } k \\text{ variables explicativas}\\\\\r\n\\text{son colectivamente √∫tiles para explicar.}\r\n\\]\r\nCon la hip√≥tesis de normalidad se demuestra que:\r\n\\[\r\n\\hat{\\beta} \\sim N_{k+1} \\bigg(\\beta, \\sigma^2(\\mathbb{X}^T\\mathbb{X})^{-1}\\bigg)\r\n\\]\r\nDefinici√≥n. (Coeficiente de determinaci√≥n \\(R^2\\))\r\n\\[\r\nR^2 = \\frac{RegSS}{TSS} = \\frac{\\displaystyle \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2}{\\displaystyle \\sum_{i=1}^n(y_i - \\bar{y})^2}\r\n\\]\r\nTristemente üò¢en MLR (Multiple Linear Regression)ya no se cumple que\r\n\\[\r\nR^2 = r^2\r\n\\]\r\nSin embargo, s√≠ se cumple que\r\n\\[\r\nR^2 = \\bigg[\\frac{\\displaystyle \\sum_{i=1}^n(y_i - \\bar{y})(\\hat{y}_i - \\bar{y})}{\\underbrace{\\sqrt{\\displaystyle \\sum_{i=1}^n(y_i - \\bar{y})^2\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2}}_{\\text{Es el cuadrado de la correlaci√≥n} \\\\ \\text{muestral entre }y \\text{ y } \\hat{y}}}\\bigg]^2\r\n\\]\r\n\\[\r\nF = \\frac{n-k-1}{k} \\cdot \\frac{R^2}{1-R^2}\r\n\\]\r\nAhora s√≠ a construir el intervalo de confianza.\r\nIntervalo de confianza para \\(\\hat{\\beta}\\)\r\nEst√° dado por\r\n\\[\r\n\\hat{\\beta}_j \\pm t_{n-(k+1), \\frac{\\alpha}{2}} \\sqrt{S^2 \\bigg(\\mathbb{X}^T\\mathbb{X}\\bigg)^{-1}_{j-1, j+1}}\r\n\\]\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T20:55:47-05:00",
    "input_file": "regresin-lineal-mltiple.utf8.md"
  },
  {
    "path": "posts/2021-05-25-regresin-lineal-simple/",
    "title": "Regresi√≥n lineal simple",
    "description": "Repaso de regresi√≥n lineal con una variable explicativa.",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-25",
    "categories": [
      "regresion"
    ],
    "contents": "\r\n\r\nContents\r\nModelos lineales\r\nRegresi√≥n cl√°sica\r\nModelos lineales generalizados\r\nModelos generalizados aditivos (GAM‚Äôs)\r\n\r\nRegresi√≥n Lineal Simple\r\nSuposiciones del modelo\r\nTSS: Total SS\r\nResidual SS √≥ Error SS\r\nRegSS: Regression SS\r\nLa prueba F\r\n\r\nPropiedades de \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\)\r\nIntervalos de confianza para \\(\\beta_j\\)\r\n\r\n\r\n\r\nModelos lineales\r\nEs una t√©cnica supervisada. La respuesta ser√° \\(y\\) mientras que las variables explicativas, predictivas o covariables se denotar√°n por \\(x_1, x_2, ..., x_p\\). La familia de los modelos lineales es muy vers√°til.\r\nRegresi√≥n cl√°sica\r\n\\[\r\ny = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\r\n\\]\r\nHay una relaci√≥n lineal entre \\(y\\) y \\(x_1, x_2, ..., x_p\\)\r\nModelos lineales generalizados\r\n\\[\r\ng(y) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p\r\n\\]\r\nDonde \\(g\\) es una funci√≥n.\r\nEjemplos por excelencia de este tipo de rtegresi√≥n:\r\nRegresi√≥n log√≠stica.\r\nRegresi√≥n Poisson.\r\nModelos generalizados aditivos (GAM‚Äôs)\r\n\\[\r\ny = \\beta_0 + \\beta_1 g(x_1) +\\beta_2 g(x_2) + ... + \\beta_p g(x_p)\r\n\\]\r\nEn general nos interesan al menos 5 cosas (independientemente del modelo lineal a trabajar).\r\n¬øC√≥mo se estiman las \\(\\beta ' ^s\\)?\r\n¬øAlgunas propiedades que tienen los estimadores \\(\\hat{\\beta}'^s\\)?\r\nInsesgado\r\nVarianza\r\nIntervalo de confianza\r\n\r\nPredicci√≥n \\(\\hat{y}_i\\)\r\nBondad de ajuste, es decir ¬ø\\(y_i \\approx \\hat{y_i}\\)?\r\n¬øCu√°les de entre \\(x_1, ..., x_p\\) son importantes para determinar la relaci√≥n con \\(y\\)? Selecci√≥n de variables o feature engineering.\r\nRegresi√≥n Lineal Simple\r\nSuposiciones del modelo\r\nEl modelo SLR (Simple Linear Regression) se basa en algunas suposiciones:\r\n\\[\r\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i,\\\\ i = 1,...,n\\text{ es decir, el n√∫mero de observaciones es }n\r\n\\]\r\n\\(y_i'^s\\) son realizaciones de variables aleatorias. Los valores \\(x_i\\) son no-aleatorios.\r\nLas cantidades \\(\\epsilon_1, ..., \\epsilon_n\\) representan errores aleatorios que son independientes entre s√≠ y adem√°s:\r\n\\(\\mathbb{E}(\\epsilon_i) = 0, i = 1,...,n\\)\r\n\\(Var(\\epsilon_i) = \\sigma^2\\) Conocida como Hip√≥tesis de homocedasticidad.\r\nBajo estos supuestos se tiene que\r\n\\(\\mathbb{E}(y_i) = \\beta_0 + \\beta_1x_i\\)\r\n\\(Var(y_i) = Var(\\beta_0 + \\beta_1x_i + \\epsilon_i) = Var(\\epsilon_i) = \\sigma^2\\)\r\nCosas que ya deber√≠amos saber:\r\nEstimaci√≥n de \\(\\beta_0\\) y \\(\\beta_1\\) por m√≠nimos cuadrados.\r\n\\[\r\nSS(\\beta_0, \\beta_1) := \\sum_{i=1}^n[\\underbrace{y_i}_{\\text{observado}} - \\underbrace{(\\beta_0 + \\beta_1x_i)}_{\\text{Supuesto}}]^2\r\n\\]\r\nSe seleccionan \\(\\beta_0, \\beta_1\\) de tal forma que se minimice \\(SS(\\beta_0, \\beta_1)\\) (lo cual es un problema cl√°sico de optimizaci√≥n).\r\n\\[\r\n\\hat{\\beta}_1 = \\frac{\\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\r\n\\]\r\n\\[\r\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta_1}\\bar{x}\r\n\\]\r\ndonde\r\n\\[\r\nS_{xy} = \\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y}) = \\displaystyle \\sum_{i=1}^n x_iy_i - n\\bar{x}\\bar{y}\\\\\r\nS_{xx} = \\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})^2 = \\displaystyle \\sum_{i=1}^n x_i^2 - n\\bar{x}^2\r\n\\]\r\nUna propiedad interesante de estos estimadores es la siguiente\r\n\\[\r\n\\hat{\\beta}_1 = r_{xy}\\frac{S_y}{S_x}\r\n\\]\r\ndonde\r\n\\[\r\nS_y := \\displaystyle \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}(y_i-\\bar{y})^2 }\\\\\r\nS_x := \\displaystyle \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}(x_i-\\bar{x})^2 }\\\\\r\n\\text{Coeficiente de correlaci√≥n muestral entre }x \\text{ y } y:\\\\\r\nr_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx} S_{yy}}}\r\n\\]\r\nEste resultado ‚Äújustifica‚Äù el caso de la correlaci√≥n como medida de asociaci√≥n lineal y el dibujo que nos encanta ‚ù§Ô∏è\r\n\r\nYa con estos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) en la mano, podemos definir \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) como el ajustado y tambi√©n los residuales como\r\n\\[\r\n\\hat{\\epsilon}_i = e_i := y_i - \\hat{y}_i\r\n\\]\r\nImportante\r\n\\[\r\n\\text{Residuales} \\neq \\text{Errores aleatorio}\\\\\r\n\\underbrace{e_i}_{\\text{Calculables, reales}}\r\n\\space \\space \\space¬†\\space \\space \\space\\space\\space\\space\r\n \\underbrace{\\epsilon_i}_{\\text{Variables aleatorias}\\\\\\text{no observables}}\r\n\\]\r\nEn los cursos se demuestra que\r\n\\(\\displaystyle \\sum_{i=1}^n e_i = 0\\)\r\n\\(\\displaystyle \\sum_{i=1}^n x_ie_i = 0\\)\r\nPara hacer inferencia, tenemos que hacer algunas suposiciones, la m√°s com√∫n es \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) y adem√°s que \\(\\epsilon_1, \\epsilon_2, ..., \\epsilon_n\\) son i.i.d‚Äôs.\r\nEsta suposici√≥n nos lleva a que\r\n\\[\r\ny_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\r\n\\]que es algo fuerte de suponer.\r\nTenemos que ‚Äúevaluar‚Äù qu√© tan bueno es el modelo, es decir, si incorporar a la variable \\(x\\) para explicar \\(y\\) es valioso. Entonces\r\n\\[\r\n\\underbrace{\\displaystyle \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}_{\\text{Modelo de}\\\\ \\text{regresi√≥n}} < \\underbrace{\\displaystyle \\sum_{i=1}^n (y_i - \\bar{y}_i)^2}_{\\text{Modelo naive}\\\\ \\text{iid}}\r\n\\]\r\nUna descomposici√≥n popular es:\r\n\\[\r\ny_i - \\bar{y} = y_i - \\hat{y}_i + \\hat{y}_i - \\bar{y} \\\\\r\n\\]\r\n\\[\r\n\\Rightarrow \\text{ (Se demuestra)} \\\\\r\n\\]\r\n\\[\r\n\\underbrace{ \\displaystyle \\sum_{i=1}^n (y_i - \\bar{y})^2}_{\\text{TSS}} = \\underbrace{ \\displaystyle \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}_{\\text{RSS √≥}\\\\\r\n\\text{Error S}} + \\underbrace{ \\displaystyle \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}_{\\text{Reg SS}}\r\n\\]\r\nTSS: Total SS\r\n\\(\\displaystyle \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1) S_y^2\\)\r\nVariaci√≥n de la respuesta con respecto a su media muestral \\(\\bar{y}\\)\r\nCantidad de variabilidad inherte en las respuestas antes de realizar la regresi√≥n.\r\nResidual SS √≥ Error SS\r\n\\[\r\nRSS = \\displaystyle \\sum_{i=1}^n (y_i - \\hat{y})^2 \r\n\\]\r\nVariaci√≥n de la respuesta con respecto a la rexta de regresi√≥n\r\nMide la bondad de ajuste de LSR. Mientras m√°s bajo, mejor \\(\\downarrow\\) üòÑ\r\nMide la cantidad de variabilidad de la respuesta que no es explicada a√∫n despu+es de introducir \\(x\\)\r\n\r\nRegSS: Regression SS\r\n\\(RegSS = \\displaystyle \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2\\)\r\nVariaci√≥n explicada por el modelo SLR i.e.¬†el conocimiento de \\(x\\) v.s. el deconocimiento de \\(x\\).\r\nMide qu√© tan efectivo es el modelo SLR en explicar la variaci√≥n en \\(y\\). (Incorporar \\(x\\) v.s. no incorporar \\(x\\)).\r\n\r\nTrivialmente \\(RSS < TSS\\)\r\nComo \\(TSS\\) permanece fijo, mientras m√°s grande sea \\(RegSS\\), m√°s peque√±o ser√° \\(RSS\\)\r\n\\[\r\nRegSS \\text{ grande & } RSS \\text{ peque√±o } \\rightarrow \\text{ fue buena idea introducir }x \r\n\\]\r\nLo anterior motiva la definici√≥n de \\(R^2\\) a.k.a. coeficiente de determinaci√≥n\r\n\\[\r\nR^2 := \\frac{RegSS}{TSS} = 1- \\frac{RSS}{TSS}\r\n\\]\r\nSe requiere que \\(R^2\\) sea lo m√°s cercano posible a 1.\r\nMientras m√°s grande sea el valor de \\(R^2\\), m√°s efectiva ser√° la recta de regresi√≥n en reducir la varianza de \\(y\\).\r\nEn SLR hay relaciones entre \\(\\hat{\\beta}_1\\) y \\(RSS\\)\r\n\\(RegSS = \\hat{\\beta}_1^2 S_{xx}\\)\r\n\\(RSS = S_{yy} - \\hat{\\beta}_1^2S_{xx}\\)\r\nComo \\(S_{xx}\\) no cambia, entonces si \\(\\hat{\\beta}_1\\) es grande, sucede que \\(RegSS\\) es grande y por tanto fue buena idea introducir \\(x\\).\r\nTambi√©n se puede demostrar que en SLR\r\n\\[\r\nR^2 = \\underbrace{r_{xy}^2}_{\\text{Cuadrado del}\\\\ \\text{coeficiente de}\\\\ \\text{Correlaci√≥n} \\\\ \\text{muestral}} = \\bigg(\\frac{S_{xx}}{\\sqrt{S_{xx}\\cdot S_{yy}}}\\bigg)^2\r\n\\]\r\nOtra cantidad popular en el an√°lisis de regresi√≥n es\r\n\\[\r\n\\boxed{MSE := \\frac{RSS}{n-2} = \\frac{\\displaystyle \\sum_{i=1}^ne_i^2}{n-2} =: S^2} \r\n\\]\r\nAdem√°s \\(S^2\\) es un estimador insesgado de \\(\\sigma^2\\), es decir \\(\\mathbb{E}(S^2) = \\sigma^2\\)\r\nPara probar formalmente si \\(RegSS = \\displaystyle \\sum_{i = 1} ^n ( \\hat{y}_i - \\bar{y})^2\\) es suficientemente grande, se lleva a cabo\r\nLa prueba F\r\n\\[\r\n\\underbrace{H_0: \\beta_1 = 0}_{\\text{agregar } x \\text{ no} \\\\ \\text{redujo la variabilidad} \\\\ \\text{de }y} \\space \\space \\space \\space \\space \\space \\space H_a: \\beta_1 \\neq 0\r\n\\]\r\nEstad√≠stica de prueba \\(F := \\frac{RegSS/1}{Rss/(n-2)}\\)\r\nSea \\(F_{1, n-2, \\alpha} \\in \\mathbb{R}\\) tal que \\(\\mathbb{P}(F_{1,n-2}> \\underbrace{F_{1, n-2, \\alpha}}_{\\text{upper cuantil}}) = \\alpha\\)\r\nRegla de decisi√≥n\r\nSi \\(\\underbrace{F}_{\\text{estad√≠stica}\\\\ \\text{de prueba}} > F_{1, n-2, \\alpha}\\) entonces se rechaza \\(H_0\\)\r\nO bien a trav√©s del \\(p-value\\)\r\n\\[\r\n\\text{Si } \\mathbb{P}(F_{1,n-2} > F) < \\alpha, \\text{ entonces se rechaza } H_0\r\n\\]\r\nMientras m√°s peque√±o sea el \\(p-value\\), se tendr√° evidencia m√°s fuerte para rechazar \\(H_0\\)\r\n\r\nUna relaci√≥n ‚Äúbonita‚Äù entre \\(F\\) y \\(R\\) (en SLR)\r\n\\[\r\nF = \\frac{RegSS/1}{Rss/(n-2)} = (n-2)\\frac{R^2}{1-R^2} = (n-2)\\frac{r_{xy}^2}{1-r_{xy}^2}\\\\\r\n\\text{Obs: la aplicaci√≥n } R^2 \\mapsto F = (n-2)\\frac{R^2}{1-R^2} \\text{ es creciente.}\r\n\\]\r\nPropiedades de \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\)\r\nSi \\(\\epsilon \\sim N(0, \\sigma^2)\\), entonces \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) tienen tambi√©n distribuci√≥n Gaussiana:\r\n\\[\r\n\\mathbb{E}(\\hat{\\beta_0}) = \\beta_0\\\\\r\n\\mathbb{E}(\\hat{\\beta_1}) = \\beta_1\\\\\r\n\\text{Es decir que son estimadortes insesgados}\\\\\r\nVar(\\hat{\\beta_0}) = \\sigma^2 \\bigg(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\bigg) \\\\\r\nVar(\\hat{\\beta_1}) = \\frac{\\sigma^2}{S_{xx}}\\\\\r\nCov(\\hat{\\beta_0}, \\hat{\\beta_1}) = \\frac{-\\bar{x}\\sigma^2}{S_{xx}}\r\n\\]\r\nLas desviaciones est√°ndar estimadas de \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\) se denotan como \\(SE(\\hat{\\beta_0})\\) y \\(SE(\\hat{\\beta_1})\\), respectivamente y se conocen como errores est√°ndar.\r\nSon medidas de la confiabilidad √≥ precauci√≥n de los LSE¬¥s\r\nDe donde\r\n\\[\r\nSE(\\hat{\\beta_0}) = \\sqrt{S^2 \\bigg(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\bigg)} \\\\\r\nSE(\\hat{\\beta_1}) = \\sqrt{\\frac{S^2}{S_{xx}}}\r\n\\]\r\n\\(S^2 \\mapsto SE(\\hat{\\beta}_0^2)\\) es creciente.\r\n\\(S^2 \\mapsto SE(\\hat{\\beta}_1^2)\\) es creciente.\r\n\\(S_{xx} \\mapsto SE(\\hat{\\beta}_0^2)\\) es decreciente.\r\n\\(S_{xx} \\mapsto SE(\\hat{\\beta}_1^2)\\) es decreciente.\r\nRecordemos que \\(S_{xx} = \\displaystyle \\sum_{i=1}^n(x_i-\\bar{x})^2\\)\r\n\r\nLo anterior graficamente se puede ver as√≠\r\n\r\nLos errores est√°ndar ser√°n peque√±os si las observaciones muestran gran tendencia a estar cerca de la recta de regresi√≥n y si los valores observados de la variable explicativa (i.e.¬†\\(x\\)) est√°n m√°s ‚Äúdispersos‚Äù a lo largo del eje \\(x\\) (es decir, \\(S_{xx}\\) grande).\r\nPuede suceder que exista m√°s dispersi√≥n pero eso no es garant√≠a de un ‚Äúbuen ajuste‚Äù.\r\n\r\n\r\nIntervalos de confianza para \\(\\beta_j\\)\r\nA partir de los errores est√°ndar ya definidos, se puede demostrar que los intervalos del \\((1-\\alpha)\\%\\) de confianza para \\(\\beta_j\\) es:\r\n\\[\r\n\\hat{\\beta}_j \\pm \\underbrace{t_{n-2, \\frac{\\alpha}{2}}}_{\\text{upper cuantil}\\\\ \\text{al nivel } \\frac{\\alpha}{2} \\text{ de una} \\\\ \\text{distribuci√≥n } t_{(n-2)}} \\cdot SE(\\hat{\\beta}_j), \\space \\space \\space \\space \\space i = 0,1\r\n\\]\r\nEn general se pueden plantear hip√≥tesis de la siguiente manera:\r\n\\(H_0: \\beta_j = d\\) v.s. \\(H_1: \\beta_j \\neq d\\)\r\n\\(H_0: \\beta_j = d\\) v.s. \\(H_1: \\beta_j > d\\)\r\n\\(H_0: \\beta_j = d\\) v.s. \\(H_1: \\beta_j < d\\)\r\nDonde \\(d \\in \\mathbb{R}\\) especificado por el usuario.\r\nPara este tipo de contraste us√°bamos la prueba \\(t\\).\r\n\\[\r\nt(\\hat{\\beta}_j) = \\frac{\\hat{\\beta}_j - d}{SE(\\hat{\\beta}_j)}, \\space \\space \\space \\space  \\space \\space j = 0,1 \r\n\\]\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T12:33:02-05:00",
    "input_file": "regresin-lineal-simple.utf8.md"
  },
  {
    "path": "posts/2021-05-21-regresin-ordinal/",
    "title": "Regresi√≥n ordinal",
    "description": "Definici√≥n y resultados",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [
      "regresion"
    ],
    "contents": "\r\n\r\nContents\r\nModelo logit acumulado\r\nModelo de odds proporcionales\r\nRespuesta de conteo\r\nModelo de regresi√≥n Poisson\r\n\r\nEstimaci√≥n m√°ximo veros√≠mil\r\n\r\nLa variable respuesta es categ√≥rica ordinal.\r\nComo antes \\(\\pi_j = \\mathbb{P}(y = j), j = 1,2,...,c\\)\r\nSolo \\(c-1\\) de estas probabilidades son libres pues \\(\\pi_1 + \\pi_2 +...+\\pi_c = 1\\) y estas probabilidades se requieren modelar en t√©rminos de variables explicativas.\r\nSe estudiar√°n 2 propuestas para este tipo de variables\r\nModelos logit acumulado\r\n\r\nModelo de odds proporcionales\r\n\r\n\r\nModelo logit acumulado\r\nUsa el link logit para explicar las probabilidades acumuladas:\r\n\\[\r\n\\tau_j := \\pi_1 + ... + \\pi_j, j = 1, ..., c-1\r\n\\]\r\nEs decir,\r\n\\[\r\n\\log\\bigg(\\frac{\\tau_j}{1-\\tau_j}\\bigg) = \\underline{x}^T\\beta_j = \\beta_{0j} + \\beta_{1j}x_1 + ... + \\beta_{kj}x_k \r\n\\]\r\nDe nuevo, hay \\(c-1\\) regresiones\r\n\r\n\\(\\log\\bigg(\\frac{\\tau_j}{1-\\tau_j}\\bigg) = \\underline{x}^T\\beta_j\\) se puede escribir como\r\n\\[\r\n\\log\\bigg(\\frac{\\pi_1+...+\\pi_j}{\\pi_{j+1} + ... + \\pi_c}\\bigg) = \\underline{x}^T\\beta_j, j = 1,2,...,c-1\r\n\\]\r\nPor ejemplo, si \\(c = 3\\) y hay 4 variables explicativas \\(x_1, x_2,x_3,x_4\\), se tienen 2 ecuaciones:\r\n\\[\r\n\\log\\bigg(\\frac{\\pi_1}{\\pi_2+\\pi_3}\\bigg) = \\beta_{01} +\\beta_{11}x_1 + \\beta_{21}x_2 + \\beta_{31}x_3 + \\beta_{41}x_4\\\\\r\n\\log\\bigg(\\frac{\\pi_1+\\pi_2}{\\pi_3}\\bigg) = \\beta_{02} +\\beta_{12}x_1 + \\beta_{22}x_2 + \\beta_{32}x_3 + \\beta_{42}x_4\r\n\\]\r\nComo antes, se puede escribir \\(\\tau_j\\) como:\r\n\\[\r\n\\tau_j = \\frac{1}{1+\\exp\\{-\\underline{x}\\beta_j\\}}\r\n\\]\r\nModelo de odds proporcionales\r\nEs un caso particular del modelo logit acumulativo en el que \\(\\beta_{0j}\\) (el intercepto) var√≠a para cada \\(j\\) pero los otros coeficientes de regresi√≥n no dependen de \\(j\\).\r\nLas ecuaciones del modelo son:\r\n\\[\r\n\\log\\bigg(\\frac{\\tau_j}{1-\\tau_j}\\bigg) = \\underbrace{\\beta_{0j}}_{\\text{cambia para cada } j } + \\beta_1x_1 + ... + \\beta_kx_k, j = 1,...,c-1\r\n\\]\r\nDe nuevo, hay \\(c-1\\) regresores.\r\nEstas \\(c-1\\) ecuaciones tienen diferentes interceptos pero la misma pendiente con respecto a cada variable explicativa.\r\nRespuesta de conteo\r\nSe considerar√°n variables respuesta que representan conteos de cierto evento de referencia.\r\nModelo de regresi√≥n Poisson\r\nPara variables de conteo, una elecci√≥n popular es la distribuci√≥n Poisson\r\n\\[\r\ny \\backsim Poisson(\\mu) \\text{ con } g(\\mu) = \\underline{x}^T\\beta\r\n\\]\r\nPara la distribuci√≥n Poisson, la funci√≥n link can√≥nica es la funci√≥n \\(g(\\mu) = \\log(\\mu)\\)\r\nDe aqu√≠ que \\(\\mu = \\exp\\{\\underline{x}^T\\beta\\}\\)\r\nRegresi√≥n:\r\nRespuesta\r\n\r\nLink\r\nPoisson:\r\nPoisson\r\n&\r\nLog\r\n\r\nOffset: En estudios de datos de conteo, los conteos observados \\(y_1,...,y_n\\) pueden no ser directamente comparables entre s√≠, debido a sus exposures. Por ejemplo,\r\nEl n√∫mero de accidentes en un seguro de autom√≥viles depende del n√∫mero de veh√≠culos asegurados y el plazo de la cobertura.\r\nEl n√∫mero de muertes en un estudio de mortalidad se incrementa con el n√∫mero de sujetos y la duraci√≥n del estudio.\r\n\r\n\\(\\log(\\mu_i) = \\underbrace{\\log(E_i)}_{\\text{offset}} + \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k\\) es decir, se agrega un t√©rmino de ‚Äúexposici√≥n‚Äù, es decir \\(\\mu_i = E_i \\cdot exp\\{\\underline{x}^T_i \\beta\\}\\)\r\n\\(\\log(E_i)\\) se puede pensar como un intercepto observation-specific conocido.\r\nEstimaci√≥n m√°ximo veros√≠mil\r\n\\[\r\nL(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i !} \\propto  \\prod_{i=1}^n e^{-\\mu_i} \\mu_i^{y_i} = \\prod_{i=1}^n e^{-\\exp\\{-\\underline{x}_i^T\\beta\\}} (\\exp\\{-\\underline{x}_i^T\\beta\\})^{y_i} \r\n\\]\r\nEntonces\r\n\\[ \r\nl(\\beta) = \\sum_{i=1}^n\\bigg(-e^{-\\underline{x}_i^T\\beta} + y_i (\\underline{x}_i^T\\beta)\\bigg) + cte\r\n\\]\r\nDe aqu√≠ que\r\n\\[\r\n\\frac{d}{d\\beta}l(\\beta) = \\sum_{i=1}^n\\bigg(-e^{-\\underline{x}_i^T\\beta} + y_i \\underline{x_i}\\bigg) = \\sum_{i=1}^n(y_i -\\mu_i)\\underline{x_i}\\\\\r\n\\frac{d}{d\\beta}l(\\beta) = 0 \\iff \\underbrace{\\sum_{i=1}^n(y_i -\\mu_i)\\underline{x_i} = \\underline{0}}_{\\text{Se resuelve con respecto de }\\beta \\\\ \\text{que aparece en las }\\mu_i'^s\\text{ pues} \\\\ \\mu_i = \\exp\\{\\underline{x}_i^T\\beta\\}}\\]\r\nLa soluci√≥n \\(\\hat{\\beta}\\) genera las medias ajustadas\r\n\\[\\hat{ \\mu}_i = \\exp\\{\\underline{x}_i^T\\beta\\}\\]\r\nQue satisface\r\n\\[\r\n\\sum_{i=1}^n(y_i -\\hat{\\mu}_i)\\underline{x_i} = \\underline{0}\\\\\r\n\\text{Esto implica que } \\sum_{i=1}^ny_i = \\sum_{i=1}^n \\hat{\\mu}_i\\\\\r\n\\text{Entonces } \\sum_{i=1}^ne_i = \\sum_{i=1}^n (y_i -\\hat{\\mu}_i) = 0\\\\\r\n\\text{i.e. la suma de los residuales es } 0\r\n\\]\r\nLa correspondiente matriz de informaci√≥n\r\n\\[\r\nI(\\beta) = -E\\bigg(\\frac{d^2}{d\\beta d\\beta^T} l(\\beta)\\bigg) = \\sum_{i=1}^{n} \\mu_i \\underbrace{(\\underline{x}_i\\underline{x}_i^T)}_{\\text{una matr√≠z}}\r\n\\]\r\ndepende de \\(\\beta\\) a trav√©s de las \\(\\mu_i¬¥^s\\)\r\nPor ejemplo, cuando hay una sola variable explicativa.\r\n\\[\r\n\\log(\\mu_i) = \\beta_0+\\beta_1x_i, i = 1,2,...,n = (1\\space \\space\\space\\space x_i)  \\binom{\\beta_0}{\\beta_1}\r\n\\]\r\nA partir de la ecuaci√≥n de m√°xima verosimilitud\r\n\\[\r\n\\sum_{i=1}^n (y_i - \\hat{\\mu_i})\\underline{x}_i = \\underline{0} \\iff \\binom{\\sum_{i=1}^n (y_i - \\hat{\\mu_i})}{\\sum_{i=1}^n x_i(y_i - \\hat{\\mu_i})} = \\binom{0}{0}\r\n\\]\r\nEs decir,\r\n\\[\r\n\\begin{cases}\r\n\\displaystyle \\sum_{i=1}^n y_i = \\sum_{i=1}^n \\hat{\\mu}_i\\\\\r\n\\\\\r\n\\displaystyle \\sum_{i=1}^n x_i e_i = 0\r\n\\end{cases}\r\n\\]\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-21T20:19:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-20-diagnstico-de-modelos/",
    "title": "Diagn√≥stico del modelo",
    "description": "¬øQu√© tan viable es el uso de los modelos?",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-20",
    "categories": [
      "regresion",
      "diagnostico"
    ],
    "contents": "\r\n\r\nContents\r\nResiduales estandarizados y studentizados\r\nIdentificaci√≥n de outliers\r\nDetecci√≥n de relaciones no-lineales omitidas\r\nPuntos de influencia\r\nLeverage\r\nDistancia de Cook\r\n\r\n\r\nRaw-residual: \\(e_i = y_i - \\hat{y}_i\\) idealmente, √©ste aproxima a \\(\\epsilon_i\\) (no observable). Refleja la cantidad de variaci√≥n que est√° a√∫n presente a√∫n en el conocimiento de las variables explicativas.\r\nSi el modelo se ajusta adecuadamente, los residuales deben ser structure-less.\r\nSi \\(|e_i|\\) es ‚Äúgrande‚Äù no hay un buen ajuste de modelo\r\n\\[\r\n|e_i|\\text{ grande } \\rightarrow \\text{Mal ajuste del modelo}\\\\\r\n\\text{¬øQu√© tan grandes?}\r\n\\]\r\nPara todo \\(i \\in \\{1, ..., n\\}\\)\r\n\\[\r\nVar(\\epsilon_i) = \\sigma ^ 2\\\\\r\nVar(e_i) = \\sigma ^ 2(1-h_{ii})\r\n\\]\r\nSe puede ver esto de forma matricial\r\n\\[\r\ne = y -\\hat{y} = y - \\mathbb{X}\\hat{\\beta} = y - \\underbrace{[\\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T]}_{\\mathbb{H}}y\\\\\r\n= y - \\mathbb{H}y\\\\\r\n=\\underbrace{(I-\\mathbb{H})}_{\\text{sim√©trica e idempotente}}y\r\n\\]\r\n\r\n\\(\\mathbb{H} = \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T \\rightarrow\\) matriz sombrero.\r\n\\(Var(e) = (I - \\mathbb{H})\\sigma ^2 I (I-\\mathbb{H})^T = \\sigma^2(I-\\mathbb{H})(I-\\mathbb{H})^T\\)\r\n\\(Var(e_i) = \\sigma^2(1-\\underbrace{h_{ii}}_{\\text{leverage}}) \\rightarrow\\) Leverage grande \\(\\Rightarrow\\) Varianza baja.\r\nA \\(h_{ii}\\) se le conoce como leverage para la observaci√≥n \\(i\\).\r\nComo \\(\\sigma^2\\) es desconocido, se puede estimar a partir del MSE \\(\\sigma^2\\)\r\n\\[\r\n\\hat{Var}(e_i) = S^2(1-h_{ii}), i = 1, 2, ..., n\r\n\\]\r\nResiduales estandarizados y studentizados\r\nLos raw-residuals no son comparables entre s√≠ y no es f√°cil interpretarlos directamente.\r\nResiduales estandarizados\r\n\\[\r\ne^{st}_{i} := \\frac{e_i}{\\sqrt{S^2(1-h_{ii})}}\r\n\\]\r\nSi el modelo de regresi√≥n es correcto, los \\(e^{st}_{i}¬¥^s\\) tienen aproximadamente la misma varianza (esto los hace comparables) y aproximadamente tienen distribuci√≥n normal/gaussiana est√°ndar.\r\n\\[\r\ne^{st}_{i} \\dot{\\backsim} N(0,1)\r\n\\]\r\n\r\nResiduales studendizados\r\n\\[\r\ne^{stud}_{i} := \\frac{e_i}{\\sqrt{S_{(i)}^2(1-h_{ii})}}\r\n\\]\r\ndonde \\(S_{(i)}^2\\) es el MSE del modelo de regresi√≥n con la i-√©sima observaci√≥n excluida\r\n\\[\r\ne^{stud}_{i} \\backsim t_{n-(k+1)} \\text{ (exacto)}\r\n\\]\r\nEl uso de \\(S^2_{(i)}\\) quita el efecto de la i-√©sima observaci√≥n; de esta forma la i-√©sima observaci√≥n afecta al numerador de \\(e_i^{stud}\\) no su denominador.\r\nIdentificaci√≥n de outliers\r\n\\(\\hat{y}_i\\) es extremo si\r\n\\[\r\n|e^{st}_i| = \r\n\\begin{cases}\r\n\\geq2, \\text{ Frees}\\\\\r\n\\geq 3, \\text{ James estad} \r\n\\end{cases}\r\n\\]\r\n¬øQu√© hacer con outliers?\r\nBorrarlo\r\n\r\nConservarlo pero comentarlo\r\n\r\nHacer el modelo con y sin el outlier\r\n\r\n\r\nDetecci√≥n de relaciones no-lineales omitidas\r\nSi el modelo de regresi√≥n se especific√≥ adecuadamente, los residuales no deben mostrar patrones regulares.\r\nSe puede graficar \\(e\\) (vertical) v.s. alg√∫n predictor \\(x_j\\)\r\nSi hay patr√≥n sistem√°tico es indicacci√≥n de que se requiere usar informaci√≥n adicional para mejorar el modelo.\r\n\r\nEjemplo\r\nEn un SLR \\(y = \\beta_0 + \\beta_1x + \\epsilon\\) la gr√°fica \\(e\\) v.s. \\(x\\) muestra forma de \\(U\\) sugiere que los residuales son cuadr√°ticos en \\(x\\). Es decir\r\n\\[y -\\beta_0 - \\beta_1 \\thickapprox e \\thickapprox \\underbrace{\\gamma_0 + \\gamma_1 x +\\gamma_2x^2}_{\\text{patr√≥n cuadr√°tico}} \\\\\r\n\\Rightarrow y \\thickapprox (\\beta_0 + \\gamma_0) + (\\beta_1+\\gamma_1)x + \\gamma_2x^2\\\\\r\n\\text{que es un modelo de regresi√≥n cuadr√°tico}\\]\r\nPuntos de influencia\r\nDefinici√≥n. Se dice que una observaci√≥n muestral es un punto de influencia si su exclusi√≥n del an√°lisis de regresi√≥n lleva a conclusiones diferentes a aquellas a las que se lleg√≥ en su presencia.\r\nSe estudiar√°n 2 m√©todos para evaluar la influencia de cada observaci√≥n sobre los resultados del modelo global.\r\nLeverage\r\n\r\nDistancia de Cook\r\n\r\n\r\nEl vector de LSE‚Äôs es \\(\\hat{\\beta} = (\\mathbb{X}^T\\mathbb{X})^{-1} \\mathbb{X}\\mathbb{Y}\\)\r\n\\[\r\n\\hat{\\mathbb{Y}} = \\mathbb{X}\\hat{\\mathbb{\\beta}} =\\mathbb{X}[(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}] = \\mathbb{H}\\mathbb{Y}\\\\\r\n\\mathbb{H} = \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T \\rightarrow \\text{ matr√≠z sombrero}\\\\\r\n\\hat{\\mathbb{Y}} = \\mathbb{H}\\mathbb{Y} \\text{ \"Se multiplica a } \\mathbb{Y} \\text{ por } \\mathbb{H} \\text{ para llegar a } \\hat{\\mathbb{Y}}\r\n\\]\r\n\r\nLeverage\r\n\\(h_{ii}\\) : Leverage de la i-√©sima observaci√≥n. Representa el ‚Äúleverage‚Äù (pero √≥ ponderaci√≥n) que el i-√©simo valor respuesta ejerce sobre su propio valor ajustado.\r\nMientras m√°s grande es \\(h_{ii}\\), mayor es la influencia que \\(y_i\\) ejerce sobre \\(\\hat{y}_i\\).\r\n\\(Var(e_i) = \\sigma^2(1-h_{ii})\\). Mientras m√°s grande sea \\(h_{ii}\\), m{as peque√±o ser√° el valor de \\(Var(e_i)\\) y \\(\\hat{y}_i\\) tender√° a ser \\(y_i\\)\r\n-   Si $h_{ii} = 1$, $\\hat{y}_i$ estar√° forzado a ser $y_i$\r\n\\(h_{ii}\\) se obtiene de \\(\\mathbb{H} = \\mathbb{X}(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\) i.e.¬†s√≥lo usa a las variables explicativas, no a las variables respuesta.\r\nEl leverage es una medida de la influencia de una observaci√≥n sobre el modelo solamente en t√©rminos de sus variables explicativas.\r\n\r\nSe puede demostrar que\r\n\\[\r\nh_{ii} \\in \\bigg[\\frac{1}{n}, 1\\bigg]\\\\\r\n\\sum_{i = 1}^{n} h_{ii} = k+1 \\text{, } k \\text{ es el n√∫mero de variables explicativas.}\\\\\r\n\\\\\r\n\\text{El laverage promedio es } \\bar{h} = \\frac{k+1}{n}\r\n\\]\r\nRegla de dedo de Frees\r\nUna observaci√≥n tiene un leverage alto si\r\n\\[\r\nh_{ii} > 3\\bar{h} = \\frac{3(k+1)}{n}\\\\\r\n\\text{Laverage para SLR : } y = \\beta_0 + \\beta_1 x + \\epsilon\\\\\r\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\\\\r\n\\text{\"miestras m√°s lejos est√© } x_i \\text{ de } \\bar{x} \\text{,  m√°s grande ser√° el leverage } h_{ii} \\text{\"}\r\n\\]\r\nTanto para SLR y MLR, el leverage es una medida de lejan√≠a (remoteness) de una observaci√≥n de las observaciones restantes, en el espacio de las variables explicativas.\r\nEl leverage es un reflejo de la influencia de la observaci√≥n, pues proporciona una ‚Äúposible raz√≥n‚Äù para que una observaci√≥n sea de influencia: √©sta involucra variables an√≥malas de las variables explicativas (est√° ‚Äúlejana‚Äù del resto de las variables explicativas).\r\nImportante\r\nUna observaci√≥n con leverage alto no necesariamente es de influencia.\r\n‚ÄúUn punto de leverage alto puee o no ser de influencia‚Äù\r\n\r\nLos puntos A y B tienen leverage alto pues est√°n lejos del resto de las observaciones\r\nPero s√≥lo el punto A es de influencia pues las rectas de regresi√≥n con el punto A y sin el punto A, ser√°n muy diferentes.\r\nDistancia de Cook\r\nUna sugerencia para que un punto sea de influencia es que no s√≥lo sea outstanding en los valores de \\(x\\) si no tambi√©n en los de \\(y\\).\r\nLa distancia de cook es una medida que combina ambas dimensiones.\r\nUna manera directa de evaluar la influencia de las observaciones individuales es estudiar los cambios en la varibale respuesta ajustada si se elimina dicha observaci√≥n.\r\nSup√≥ngase que se elimina la i-√©sima observaci√≥n y se ajusta un modelo de regresi√≥n con las \\(n-1\\) observaciones restantes.\r\n\\[\r\n\\hat{y}_{j(i)}: \\text{ valor ajustado de } \\hat{y} \\text{ calculado en ausencia de la observaci√≥n } i\r\n\\]\r\nMientras m√°s grande sea \\(\\hat{y}_j - \\hat{y}_{j(i)}\\), m√°s influencial ser√° la observaci√≥n \\(i\\)\r\nDistancia de cook\r\n\\[\r\nD_i = \\displaystyle \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(k+1)S^2}\r\n\\]\r\nDonde \\(S^2\\) es el MSE calculado con el data-set completo\r\n\\(D_i \\dot{\\backsim}F\\) (que sirve para probar valores grandes o peque√±os de \\(D_i\\))\r\nFrees sugiere que \\(D_i > \\frac{1}{n}\\) significa que la observaci√≥n \\(i\\) es de influencia.\r\nLa definici√≥n de la distancia de cook requiere que se ajuste una regresi√≥n en \\((n+1)\\) data-sets:\r\n1 en el data-set completo (para obtener \\(S^2\\))\r\n\\(n\\) para cada uno de lo data-sets que excluye a la observaci√≥n \\(i\\), para calcular los \\(\\hat{y}_{j(i)}'^s\\).\r\n\r\nHay una f√≥rmula algebraicamente equivalente pero computacionalmente m√°s eficiente.\r\n\\[\r\nD_i = \\frac{1}{k+1}(e_i^{st})^2\\frac{h_{ii}}{1-h_{ii}}\r\n\\] solo requiere realizar una regresi√≥n sobre todos los datos.\r\nEn esta expresi√≥n es evidente el impacto de la respuesta \\(y_i\\) (a trav√©s de \\(e_{i}^{st}\\)) y de las covariables \\(x\\) (a trav√©s de \\(h_{ii}\\)).\r\nPara que la distancia de Cook sea grande tiene que ocurrir tanto que \\(e_{i}^{st}\\) sea grande, as√≠ como \\(h_{ii}\\), i.e.¬†la observaci√≥n es outstanding con respecto a los valores de \\(x\\) y los valores de \\(y\\).\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-20-diagnstico-de-modelos/images/d_m_1.png",
    "last_modified": "2021-05-20T23:07:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-15-construccin-del-modelo-de-regresin/",
    "title": "Construcci√≥n del modelo de regresi√≥n",
    "description": "Veremos el modelo de regresi√≥n",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-12",
    "categories": [
      "regresion"
    ],
    "contents": "\r\n\r\nContents\r\nTipos de variables explicativas\r\n¬øQu√© pasa con variables categ√≥ricas?\r\n\r\nInteracci√≥n\r\nInteracci√≥n entre las variables explicativas continuas y categ√≥ricas\r\nModelos de regresi√≥n lineal picewise\r\nModelo 1.\r\nModelo 2\r\n\r\n\r\nEl MLR proporciona mayor flexibilidad para describir la relaci√≥n entre la respuesta y las variables explicativas.\r\nEl reto ahora es c√≥mo representar diversos predictores (variables explicativas / covariables) de manera adecuada. (este ‚Äúproblema‚Äù no se presentaba en SLR)\r\nTipos de variables explicativas\r\nnum√©ricas\r\ncateg√≥ricas\r\nLa manera m√°s sencilla de representar en un modelo de regresi√≥n a una variable num√©rica es asignar un coeficiente de regresi√≥n a esta variable, i.e.\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\text{ t√©rminos que no involucran a } x + \\epsilon\r\n\\]\r\nEn esta especificaci√≥n, en una unidad de incremento en \\(x\\) se espera un incremento en \\(y\\) de \\(\\beta_1\\) unidades.\r\n\\[\r\ny = \\beta_0 + \\beta_1(x+1) + \\text{ t√©rminos que no involucran a } x + \\epsilon\r\n\\]\r\nSin embargo, hay situaciones en las que la relaci√≥n entre \\(x\\) y \\(y\\) no parece ser lineal. Entonces, podr√≠a ser deseable ‚Äúexpedir‚Äù la funci√≥n de regresi√≥n con potencias de \\(x\\), llevando a una especificaci√≥n que se conoce como regresi√≥n polinomial.\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2x^2 + ... + \\beta_mx^m + \\text{ t√©rminos que no involucran a } x + \\epsilon\r\n\\]\r\nPara alg√∫n \\(m \\in \\mathbb{N}_+\\)\r\nEn el modelo de regresi√≥n polinomialm no hay linealidad en \\(x\\) sino en los primeros par√°metros \\(\\beta_j'^s\\), por eso se considera un modelo lineal.\r\nUna pregunta que surge naturalmente es c√≥mo determinar el grado del polinomio, i.e.¬†c√≥mo determinar \\(m\\). Lo estudiaremos m√°s adelante mediante algunos m√©todos de selecci√≥n de modelos.\r\n¬øQu√© pasa con variables categ√≥ricas?\r\nHay que representarlas de manera cuantitativa, una manera de hacer esto es la siguiente\r\nPara una variable categ√≥rica con \\(r\\) niveles i.e.¬†con \\(r\\) categor√≠as (\\(r\\geq 2\\))m se necesita introducir \\(r-1\\) variables indicadoras: \\(x_1, x_2, ..., x_{r-1}\\) donde a cada una se le asigna un coeficiente de regresi√≥n (por separado), donde\r\n\\[\r\nx_i = \\begin{cases}\r\n1, \\text{si la categor√≠a es } i,\\\\\r\n0, \\text{ c.o.c.}\r\n\\end{cases}\r\n\\]\r\nEjemplo:\r\n$$ x_{} {}\\\r\nx_1 =\r\n\\[\\begin{cases}\r\n1, \\text{si } x_{\\text{cat}} \\text{ es perro},\\\\\r\n0, \\text{ c.o.c.}\r\n\\end{cases}\\]\r\n\\\r\nx_2 =\r\n\\[\\begin{cases}\r\n1, \\text{si } x_{\\text{cat}} \\text{ es caballo},\\\\\r\n0, \\text{ c.o.c.}\r\n\\end{cases}\\]\r\n$$\r\n\r\nEl √∫ltimo rengl√≥n se conoce como nivel base\r\nNivel de la variable categ√≥rica\r\n\\[\r\nx_1\r\n\\]\r\n\\[\r\nx_2\r\n\\]\r\n\\[\r\n...\r\n\\]\r\n\\[\r\nx_{r-1}\r\n\\]\r\n\\[\r\n1\r\n\\]\r\n1\r\n0\r\n\\[\r\n...\r\n\\]\r\n0\r\n\\[2\\]\r\n0\r\n1\r\n\\[\r\n...\r\n\\]\r\n0\r\n\\[\r\n...\r\n\\]\r\n\r\n\r\n\r\n\r\n\\[\r\nr-1\r\n\\]\r\n0\r\n0\r\n\\[\r\n...\r\n\\]\r\n1\r\n\\[\r\nr\r\n\\]\r\n0\r\n0\r\n\\[\r\n...\r\n\\]\r\n0\r\nEjmplo:\r\nSi \\(x = \\begin{cases} 1, \\text{ para fumadores}\\\\ 0, \\text{ para no fumadores} \\end{cases}\\) Si el estatus de consumo de tabaco es la √∫nica variable explicativa, la ecuaci√≥n del modelo de regresi√≥n es:\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 +\\beta_1x = \\begin{cases} \r\n\\beta_0 +\\beta_1, \\text{ para fumadores}\\\\\r\n\\beta_0, \\text{ para no fumadores} \\end{cases}\r\n\\]\r\nObservaci√≥n: no es necesario definir una variable indicadora \\(x' = \\begin{cases} 1, \\text{ para fumadores}\\\\ 0, \\text{ para no fumadores} \\end{cases}\\) para indicar a los no-fumadores. Si se definiera dicha variable que \\(x+x'=1\\). Dicha relaci√≥n lineal perfecta entre \\(x\\) y \\(x'\\) desestabilizar√° el proceso de estimaci√≥n, esto se conoce como colinealidad y se estudiar√° m√°s adelante.\r\nEl nivel que se excluye en la descomposici√≥n en variables indicadoras se comoce como nivel baseline √≥ nivel de referencia.\r\nSe se escoge a ‚Äúno-fumador‚Äù como baseline, el coeficiente \\(\\beta_0\\) se puede interpretar como el valor de \\(\\mathbb{E}(y)\\) cuando la observaci√≥n es ‚Äúno-fumador‚Äù y \\(\\beta_1\\) captura la diferencia promedio en \\(\\mathbb{E}(y)\\) entre un fumador y un no-fumador.\r\nEsta codificaci√≥n del estatus de fumador no es la √∫nica. Se puede asignar a ‚Äúfumador‚Äù como un nivel baseline √≥ utilizar una codificaci√≥n \\(-1/1\\). Bajo diferentes codificaciones sus estimaciones parametrales y sus interpretaciones difierir√°n, aunque las predicciones ser√°n las mismas.\r\nSi el nivel baseline es ‚Äúfumadores‚Äù \\(x' = \\begin{cases} 0, \\text{ para fumadores}\\\\ 1, \\text{ para no fumadores} \\end{cases}\\) , la ecuaci√≥n del modelo se convierte en:\r\n\\[\r\n\\mathbb{E}(y) = \\alpha_0 + \\alpha_1x = \\begin{cases} \\alpha_0, \\text{ para fumadores}\\\\ \\alpha_0 +\\alpha_1, \\text{ para no fumadores} \\end{cases}\r\n\\]\r\nEn este caso, \\(\\alpha_0\\) es el valor esperado de la respuesta para fumadores y \\(\\alpha_1\\) representa el incremento en \\(\\mathbb{E}(y)\\) para un no fumador, comparado con un fumador.\r\nPara relacionar la estimaci√≥n de par√°metros en ambas codificaciones se usa el hecho de que \\(x' = 1-x\\), entonces\r\n\\[\r\n\\mathbb{E}(y) = \\alpha_0 + \\alpha_1x' = \\alpha_0 + \\alpha_1(1-x)\\\\\r\n=\\underbrace{(\\alpha_0 + \\alpha_1)}_{\\beta_0} + \\underbrace{(-\\alpha_1)}_{\\beta_1} x\r\n\\]\r\nDe aqu√≠ que \\(\\hat{\\beta_0} = \\hat{\\alpha_0} + \\hat{\\alpha_1}\\) y \\(\\hat{\\beta_1} = -\\hat{\\alpha_1}\\)\r\nSin importar si la codificaci√≥n es \\(0/1\\) √≥ \\(1/0\\), las predicciones son las mismas:\r\n\\[\r\n\\bar{y} = \\underbrace{\\hat{\\beta_0} + \\hat{\\beta_1} \\cdot 1}_{\\text{valor ajustado}\\\\ \\text{bajo codificaci√≥n 1/0}} = (\\hat{\\alpha}_0 + \\hat{\\alpha_1}) -  \\hat{\\alpha_1} = \\hat{\\alpha}_0 = \\underbrace{\\hat{\\alpha_0} + \\hat{\\alpha_1} \\cdot 0}_{\\text{valor ajustado}\\\\ \\text{bajo codificaci√≥n 0/1}} \\\\\r\n\\bar{y} = \\underbrace{\\hat{\\beta_0} + \\hat{\\beta_1} \\cdot 0}_{\\text{valor ajustado}\\\\ \\text{bajo codificaci√≥n 1/0}}= \\hat{\\beta}_0 = \\hat{\\alpha}_0 + \\hat{\\alpha_1}  = \\underbrace{\\hat{\\alpha_0} + \\hat{\\alpha_1} \\cdot 1}_{\\text{valor ajustado}\\\\ \\text{bajo codificaci√≥n 0/1}}\\\\\r\n\\therefore \\text{Las preicciones son las mismas}\r\n\\]\r\n\r\nSi se usa la codificaci√≥n 1/-1 (en vez de 1/0 √≥ 0/1) entonces\r\n\\[\r\nx^n = \\begin{cases} 1 \\text{ para fumadores,}\\\\ -1 \\text{ para no fumadores}\\\\ \\end{cases}\r\n\\]\r\nLa ecuaci√≥n del modelo se convierte en\r\n\\[\r\n\\mathbb{E}(y) = \\gamma_0 + \\gamma_1x'' = \\begin{cases} \\gamma_0 + \\gamma_1 \\text{ para fumadores,} \\\\ \\gamma_0 - \\gamma_1 \\text{ para no fumadores}\\end{cases}\r\n\\]\r\nEsta codificaci√≥n 1/-1 tiene la ‚Äúventaja‚Äù de hacer al intercepto \\(\\gamma_0\\) el ‚Äúpromedio global‚Äù de \\(y\\) para todos los individuos ognorando el efecto de fumado/no-fumador. Adem√°s \\(\\gamma_1\\) es la cantidad que los fumadores tienen adicional al promedio y \\(\\gamma_1\\) tambi√©n es la cantidad que los no-fumadores tienen faltante al promedio.\r\nDesde el punto de vista computacional, es conveniente seleccionar al nivel m√°s com√∫n (la de mayor frecuencia) como el nivel baseline.\r\nEsto se debe a que se tendr√° muvhos \\(0'^s\\) en la matr√≠z de dise√±o y ser√° m√°s f√°cil calcular \\((\\mathbb{X}^T\\mathbb{X})^{-1}\\)\r\n\r\nInteracci√≥n\r\nHasta el momento, s√≥lo se han considerado modelos en los que la relaci√≥n entre la respuesta y la variable explicativas es aditiva.\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\r\n\\]\r\nSin embargo, tambi√©n podemos hacer una especificaci√≥n de la forma.\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3\r\n\\]\r\nAqu√≠ \\(x_3 := x_1 x_2\\) se conoce como variable de interacci√≥n y se le trata como una variable explicativa adicional con un coeficiente de regresi√≥n por separado \\(\\beta_3\\)\r\nObs√©rvese que, incrementar una unidad en x_1\r\n\\[\r\n\\beta_0 + \\beta_1(x_1+1) + \\beta_2x_2 +\\beta_3(x_1+1)x_2\\\\\r\n= \\beta_0 + \\beta_1x_1+\\beta_1 + \\beta_2x_2 +\\beta_3x_1+\\beta_3x_2\\\\\r\n=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\beta_3x_1+(\\beta_3x_2 +\\beta_1)\r\n\\]\r\nEquivalentemente\r\n\\[\r\n\\frac{d}{dx_1}\\mathbb{E}(y) = \\frac{d}{dx_1} (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_1x_2) = \\beta_1 +\\beta_3x_2\r\n\\]\r\nEs decir, el incremento es una unidad en \\(x_1\\) incrementar√° a \\(\\mathbb{E}(y)\\) en \\(\\beta_1 +\\beta_3x_2\\) (que depende de \\(x_2\\))\r\nPor lo tanto, el impacto de cada \\(x\\) var√≠a con en valor tomado por la otra variable explicativa y se dice que \\(x_1\\) y \\(x_2\\) interact√∫an entre s√≠ para afectar \\(\\mathbb{E}(y)\\).\r\n\r\nInteracci√≥n entre las variables explicativas continuas y categ√≥ricas\r\nLa interacci√≥n entre una variable categ√≥rica y una continua tiene una interpretaci√≥n geom√©trica muy importante, que no se puede dar s√≥lo con variables continuas.\r\nConsid√©rese un modelo MLR con una variable explicativa continua \\(x_1\\), una variable binaria \\(x_2\\) y una \\(x_1 x_2\\). La ecuaci√≥n del modelo es:\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 + \\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2\\\\\r\n= \\begin{cases} \r\n\\beta_0 +\\beta_1x_1, \\text{ si }x_2 = 0\\\\\r\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, \\text{ si }x_2 = 1\r\n\\end{cases}\r\n\\]\r\nSe puede ver a √©ste como dos modelos SLR por separado: una para \\(x_2=0\\) y otro para \\(x_2=1\\).\r\nObs√©rvese que ambos modelos tienen diferentes interceptos y diferentes pendientes.\r\n\r\n¬øQu√© pasa si \\(\\beta_3 = 0\\)? Es decir, si no hay interacci√≥n. La ecuaci√≥n de modelo se convierte en:\r\n\r\nPara el grupo baeline (i.e.¬†\\(x_2 = 0\\)). El incremento en una unidad es \\(x_1\\) incremento a \\(\\mathbb{E}(y)\\) a \\(\\beta_1\\)\r\nPara el grupo \\(x_2 = 1\\). El incremento es una unidad en \\(x_1\\) incrementa a \\(\\mathbb{E}(y)\\) en \\(\\beta_1 + \\beta_3\\)\r\n\r\nSi \\(\\beta_3 \\neq 0\\), el efecto de \\(x_1\\) sobre \\(y\\) difiere de acuerdo a si \\(x_2 = 0\\) √≥ \\(x_2 = 1\\) ‚Ä¶ una manifestaci√≥n de la interacci√≥n.\r\nSi \\(\\beta_3 = 0\\), la interacci√≥n desaparece y simplemente se est√°n ajustando 2 rectas paralelas (con diferentes interceptos) a los datos.\r\nEn \\(\\mathbb{E}(y) = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + \\beta_3x_1x_2\\) es poco com√∫n que\r\n\\[\r\n\\underbrace{\\beta_3 \\neq 0}_{\\text{evidenciado por una estad√≠stica grande √≥ un p-value peque√±o.}} \\text{ y } \\underbrace{\\beta_1 = \\beta_2 = 0}_{\\text{con la comesp prueba estad√≠stica}}\r\n\\]\r\nEs decir, el efecto puede ser real pero el de \\(x_1\\) y \\(x_2\\) se insignificante. En este caso se apela a lo que se conoce como ‚Äúprincipio jer√°rquico‚Äù y se incluye no s√≥lo \\(\\beta_3\\) sino tambi√©n \\(\\beta_1\\) y \\(\\beta_2\\) para facilitar la interpretaci√≥n.\r\nModelos de regresi√≥n lineal picewise\r\nEn ciertas aplicaciones puede ser deseable que la variable respuesta muestre cambios abiertos en el comportamiento sobre diferentes intervalos de la variable explicativa (que se conoce como ‚Äúrompimiento estructural‚Äù).\r\nEn el caso en el que una variable explicativa es categ√≥rica ya se explic√≥ (un quiebre por cada nievel). Esto tambi√©n se puede estudiar como una variable explicativa continua.\r\nModelo 1.\r\nConsid√©rese un modelo de variable explicativa \\(x\\).\r\nSea \\(z = 1_{\\{x\\geq c\\}}\\) para alg√∫n \\(c \\in \\mathbb{R}\\).\r\nConsid√©rese la funci√≥n de regresi√≥n:\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 + \\beta_1x + \\beta_2 z(x-c)\\\\\r\n= \\beta_0 + \\beta_1x + \\beta_2 (x-c)_+ \\text{ donde } u_+ := \\max\\{u ,0\\}\\\\\r\n=\\begin{cases} \r\n\\beta_0 + \\beta_1x \\text{ si } x<c, \\\\\r\n(\\beta_0 -\\beta_2c) + (\\beta_1 +\\beta_2)x \\text{ si } x\\geq c\r\n\\end{cases}\r\n\\]\r\nLa pendiente de la funci√≥n de regresi√≥n cambia abruptamente de \\(\\beta_1\\) a \\(\\beta_1+\\beta_2\\) en \\(x=c\\)\r\nDicho modelo se puede ver como un modelo MLR con dos variables explicativas \\(x\\) y \\((x-c)_+\\) y la inferencia se lleva a cabo como se hace normalmente.\r\nCon este modelo se obtiene una sola funci√≥n de regresi√≥n formada por 2 rectas conectadas continuamente en \\(x = c\\) (que se conoce como kink [torcedura]). Por esta raz√≥n, a este modelo se le conoce como modelo de regresi√≥n lineal pice wise.\r\n\r\nModelo 2\r\nLa funci√≥n de regresi√≥n en un modelo de regresi√≥n lineal picewise no necesita ser continua.\r\nUna funci√≥n de regresi√≥n con saltos resultar√° de ‚Äúinteractuar‚Äù una variable explicativa continua \\(x\\) con la variable dummy \\(z = 1_{\\{x > c\\}}\\)\r\nLa funci√≥n de regresi√≥n es:\r\n\\[\r\n\\mathbb{E}(y) = \\beta_0 + \\beta_1x + \\beta_2 z + \\beta_3zx\\\\\r\n=\\begin{cases} \r\n\\beta_0 + \\beta_1x \\text{ si }z = 0 \\iff x<c, \\\\\r\n(\\beta_0 +\\beta_2c) + (\\beta_1 +\\beta_3)x \\text{ si } z =1 \\iff x\\geq c\r\n\\end{cases}\r\n\\]\r\nQue consiste en dos l√≠neas rectas que generalmente no se conectan, divididas en \\(x = c\\).\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-19T23:31:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-14-clustering/",
    "title": "Clustering",
    "description": ".",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "clustering"
    ],
    "contents": "\r\n\r\nContents\r\nDefiniciones generales\r\nClustering por k medias\r\nAlgor√≠tmo: cl√∫stering por k-medias\r\n\r\nCl√∫stering jer√°rquico\r\n\r\nDefiniciones generales\r\nUn m√©todo de clustering se utiliza para agrupar un data-set heterog√©neo en subgrupos homog√©neos que se conocen como clusters. Estrategia no supervisada, es decir, no hay variable respuesta \\(y\\).\r\nRevisaremos\r\nClustering por k-medias\r\nCluestering jer√°rquico\r\n\r\nSe considera\r\nSi se conoce a priori el npumero de sibgrupos, se usa k-medias.\r\nSi no se conoce a priori el n√∫mero de subgrupos, se usa clustering jer√°rquico.\r\n\r\nUn cluster es un grupo de observaciones que son ‚Äúmuy similares‚Äù y tienen varias caracter√≠sticas en com√∫n.\r\nEntonces se quiere cuantificar la noci√≥n de ‚Äúmuy similar‚Äù\r\n\r\nLa cercan√≠a de las observaciones est√° determinada por una distancia.\r\nEn principio, la distancia Euclidiana\r\nPero pueden haber otras metricas\r\n\r\nClustering por k medias\r\nSe quiere agrupar al data-set como \\(\\textbf{k}\\) cl√∫sters disjuntos.\r\nLos datos iniciales son \\(n\\) y est√°n enumerados de 1 a \\(n\\).\r\nLas etiquetas de las observaciones se guardan en \\(C = \\{1,2,...,n\\}\\)\r\n\r\nCon clustering por k-medias se quiere determinar cl√∫sters \\(C_1, C_2, ..., C_k\\)\r\nLas etiquetas en \\(C_i\\) son observaciones que pertenecen al cl√∫ster \\(i\\).\r\nCada observaci√≥n ser√° asignada a un y s√≥lo un cl√∫ster.\r\n\\(C = C_1 \\cup C_2 \\cup ... \\cup C_k\\)\r\nSi \\(i \\neq j\\), entonces \\(C_i \\cap C_j = \\emptyset\\)\r\n\r\nCon clustering por k-medias, se quiere encontrar cl√∫sters \\(C_1,C_2,...,C_k\\) talqes que la variaci√≥n dentro de cada cl√∫ster ser√° lo m√°s peque√±a posible.\r\nMientras m√°s peque√±a sea la variaci√≥n within-cluster, mejor serpa el m√©todo de clustering. üòä\r\nLa variaci√≥n dentro de un cl√∫ster se mide usando una fucni√≥n \\(W(\\cdot)\\)\r\n\\(W(C_j)\\): Variaci√≥n entre las observaciones en el cl√∫ster \\(j\\).\r\n\r\nLa variaci√≥n total est√° dada por\r\n\\[\r\n\\text{Variaci√≥n total } = \\sum_{j = 1}^{k} W(C_j)\r\n\\]\r\nEl clustering por k-medias determinan qu√© cl√∫sters \\(C_1, ..., C_k\\) minimizan la variaci√≥n total\r\n\\[\r\n\\min_{C_1, ..., C_k}\\sum_{j=1}^{k}W(C_j)\r\n\\]\r\nUna forma com√∫n de definir \\(W(\\cdot)\\) es como la distancia Euclidiana entre las observaciones.\r\nSup√≥ngase que las observaciones \\(\\underline{x}_i, \\underline{x}_{i'}\\) est√°n en el k-√©simo cl√∫ster\r\nEsto significa que \\(i, i' \\in C_k\\)\r\n\\[\r\n\\text{Distancia al cuadrado entre } i \\text{ y } i' \\\\\r\n=\\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2\r\n\\]\r\n\r\n\\(W(C_k)\\): Distancia al cuadrado promedio entre las observaciones en el cl√∫ster \\(k\\)\r\n\\[\r\nW(C_k) = \\frac{1}{|C_k|}\\sum_{i,i' \\in C_k} \\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2\r\n\\]\r\nLos cl√∫sters \\(C_1, ..., C_k\\) se obtienen de tal forma que\r\n\\[\r\n\\min_{C_1, ..., C_k}\\bigg\\{ \\sum_{k=1}^K \\frac{1}{|C_k|}\\sum_{i,i' \\in C_k} \\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2 \\bigg\\}\r\n\\]\r\nAlgor√≠tmo: cl√∫stering por k-medias\r\nPaso preliminar, asignar aleatoriamente cada observaci√≥n a un cl√∫ster.\r\nIteraci√≥n. Repetir los siguientes pasos hasta que la asignaci√≥n del cl√∫ster no cambie entre 2 pasos consecutivos:\r\nDeterminar el centroide \\(\\bar{x}_k\\) para cada cl√∫ster \\(C_k\\) para cada \\(k = 1, 2, ..., K\\)\r\n\\[\r\n\\bar{x}_k := (\\bar{x}_{k1}, \\bar{x}_{k2},...,\\bar{x}_{kp})\r\n\\]\r\n(el promedio en el cl√∫ster por cada variable), donde:\r\n\\[\r\n\\bar{x}_{kj} = \\frac{1}{|C_k|}\\sum_{i \\in C_k} x_{ij}, \\space \\space j =1,2, ...,p\r\n\\]\r\nAsignar a cada observaci√≥n al cl√∫ster del centroide m√°s cercano. De hecho, una observaci√≥n \\(\\underline{x}_i\\) pertenece al cl√∫ster \\(k\\).\r\n\\[\r\n\\sum_{j=1}^p(x_{ij}-\\bar{x}_{k'j})^2 \\leq \\sum_{j=1}^p(x_{ij}-\\bar{x}_{k'j}) \\text{  para cada cl√∫ster } k'\r\n\\]\r\nEl algor√≠tmo minimiza la distancia de las observaciones a los centroides. Sin embargo, el problema de optimizaci√≥n establece minimizar la distancia entre las observaciones que pertenecen al mismo cl√∫ster. La siguiente expresi√≥n relaciona ambos problemas\r\n\\[\r\n\\frac{1}{|C_k|}\\sum_{i,i'\\in C_k} \\sum_{j=1}^{p}(x_{ij}-x_{i¬¥j})^2 = 2\\sum_{i \\in C_k} \\sum_{j=1}^{p}(x_{ij}-x_{i¬¥j})^2\r\n\\]\r\nCl√∫stering jer√°rquico\r\nEl m√©todo de clustering jer√°rquico no requiere que se especif√≠que el n√∫mero de cl√∫sters al inicio.\r\nEl cl√∫stering jer√°rquico utiliza un dendograma\r\n\r\nLa construcci√≥n del dendograma es la siguiente:\r\nSe tiene un conjunto de observaciones \\(\\underline{x}_1,\\underline{x}_2,...,\\underline{x}_n\\) cada una de dimensi√≥n p.\r\nEl dendograma se construye de abajo hacia arriba. Hasta abajo se ponen todas las observaciones de manera separada; esto indica que cada observaci√≥n es su propio cl√∫ster, i.e.¬†se tienen \\(n\\) cl√∫sters.\r\nDespu√©s se ‚Äúfusionan‚Äù dos cl√∫sters en uno. As√≠, en el segundo nivel del dendograma se tienen \\(n-1\\) cl√∫sters: \\(n-2\\) con un elemento y 1 con dos observaciones.\r\nEl siguiente paso es crear un tercer nivel fusionando de nuevo 2 cl√∫sters.\r\nSe contin√∫a con este proceso hasta el nivel superior en el que todas las observaciones perteneces a un cl√∫ster.\r\n\r\nHay muchas formas de hacer estas ‚Äúfusiones‚Äù de dos cl√∫sters en uno en cada nivel.\r\nEn cada paso de la construcci√≥n del dendograma se debe determinar la disimilaridad (dissimilarity)\r\nExisten diferentes medidas de disimilaridad.\r\nLinkage: Disimilaridad entre 2 grupos de observaciones.\r\nHay 4 tipos de linkage populares:\r\nCompleto\r\nIndividual (single)\r\nPromedio (average)\r\nCentroide\r\n\r\nLinkage completo. Calcula la disimilaridad entre cada punto del cl√∫ster \\(A\\) y cada punto del cl√∫ster \\(B\\). El linkage completo entre \\(A\\) y \\(B\\) es la distancia m√°xima. Para calcular este, se hacen \\(|A|\\cdot|B|\\) distancias y se toma la m√°xima.\r\nLinkage individual. Calcula la disimilaridad entre cada punto del cl√∫ster \\(A\\) y cada punto del cl√∫ster \\(B\\). El linkage individual entre \\(A\\) y \\(B\\) es la distancia m√≠nima.\r\nObservaci√≥n. Este linkage lleva a cl√∫sters ‚Äúcolgantes‚Äù (trailing cl√∫sters), cl√∫sters en los que un punto a la vez se fusionan con un single cl√∫ster.\r\n\r\n\r\nLinkage promedio. Calcula la disimilaridad entre cada punto del cl√∫ster \\(A\\) y cada punto del cl√∫ster \\(B\\). El linage promedio entre \\(A\\) y \\(B\\) es el promedio de estas distancias.\r\nLinkage centroide. Calcula el centroide de cada cl√∫ster y usa la disimilaridad entre los centroides.\r\nObservaci√≥n. Bajo este m√©todo se tiene la desventaja de que pueden ocurrir ‚Äúinversores‚Äù (la disimilaridad de la fusi√≥n ‚Äúfutura‚Äù es menor que la disimilaridad de una fusi√≥n ‚Äúpasada‚Äù, involucrando los mismos puntos).\r\n\r\nUna vez que se contruy√≥ un dendograma, se puede determinar los cl√∫sters dibujando una recta horizontal en el dendograma.\r\nrecta arriba \\(\\rightarrow\\) pocos cl√∫sters.\r\nrecta abajo \\(\\rightarrow\\) muchos cl√∫sters.\r\n\r\n\r\nWithin-cluster-variation\r\n\\[\r\nW(C_k):=\\sum_{i \\in C_k} \\sum_{j=1}^{p}(x_{ij}- \\bar{x}_{kj})^2\\\\\r\nW_k := \\sum_{k=1}^KW(C_k)\r\n\\]\r\n\\(W_k\\) debe ser peque√±a\r\n\\(k \\mapsto W_k\\) es decreciente\r\n¬øIncrementa el n√∫mero de cl√∫sters? üò¢\r\n\r\nBetween-cluster-variation\r\n\\[\r\nb_k = \\sum_{k=1}^K|C_k|\\sum_{j=1}^{p}(\\bar{x}_{kj}-\\bar{x}_{j})^2\\\\\r\n\\text{\"La suma de las distancias entre los centroides\"}\r\n\\]\r\n\\(b_k\\) debe ser grande\r\n\\(k \\mapsto b_k\\) es creciente\r\n¬øIncrementa el n√∫mero de cl√∫sters? üò¢\r\n\r\n√çndice CH\r\n\\[\r\nCH_k:=\\frac{\\frac{b_k}{k-1}}{\\frac{W_k}{n-k}}\r\n\\]\r\nDonde \\(n\\) es el n√∫mero total de observaciones en el data-set.\r\nEl valor √≥ptimo para \\(k\\) es aquel que maximiza \\(CH_k\\)\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-25T21:47:33-05:00",
    "input_file": "clustering.utf8.md"
  },
  {
    "path": "posts/2021-05-14-rboles-2/",
    "title": "√Årboles 2",
    "description": "Parte 2 de √°rboles.",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-10",
    "categories": [
      "arboles",
      "bootstrap",
      "bagging"
    ],
    "contents": "\r\n\r\nContents\r\nBootstrap, bagging y random forests\r\nBootstrap\r\nMuestras bootstrap\r\n\r\nBagging\r\n\r\nBootstrap, bagging y random forests\r\nBootstrap\r\nConsid√©rese un data set \\(D\\) con \\(n\\) observaciones, i.e.¬†\\(D\\) ser√° el conjunto de todas las observaciones en el data-set de entrenamiento.\r\n\\[\r\nD = \\{(x_1,y_1), ..., (x_n,y_n)\\}\r\n\\]\r\nLa variable respuesta es \\(y\\).\r\nEn la mayor√≠a de los modelos cl√°sicos, se obtiene un estimador puntual \\(\\hat{y}\\), bas√°ndose en el data-set.\r\nEl data set \\(D\\) es un conjunto de realizaciones independientes de una distribuci√≥n de probabilidad. Si se repite el an√°lisis con otro data-set (que es tambi√©n una relizaci√≥n de dicha distribuci√≥n de probabilidad) el estimado puntual ser√° diferente.\r\nPara estudiar el tipo de variabilidad de la predicci√≥n \\(\\hat{y}\\), se eval√∫a la sensitividad de la predicci√≥n respecto al data-set subyacente.\r\nBootstrapping es una herramineta para generar nuevos data-sets artificiales bas√°ndose en el data-set original.\r\nMuestras bootstrap\r\nEjemplo. Se tienen 5 observaciones \\((x_1,y_1), (x_1,y_1), ... (x_5,y_5)\\) cada observaci√≥n se etiqueta con un n√∫mero del 1 al 5. El n√∫mero de cada observaci√≥n se guarda en el conjunto \\(A\\).\r\nCuando se empieza aqu√≠ con el data-set original, cada n√∫mero aparece exactamente una vez en el conjunto \\(A: A=\\{1,2,3,4,5\\}\\)\r\nAhora se crear√° un nuevo conjunto de datos con 5 observaciones, seleccionando aleatoriamente 5 obs. del conjunto de datos inicial \\(A\\). Se permitir√° que la misma observaci√≥n sea incluida varias vices en el nuevo data set, i.e.¬†se seleccionar√°n aleatoriamente n√∫meros del conjunto \\(A\\) con reemplazo.\r\nLos n√∫meros en el nuevo data-set se guardar√°n en el conjunto \\(A_1\\). Por ejemplo, si \\(A_1 = \\{1, 4, 3, 5, 5\\}\\) entonces el nuevo data-set ser√° \\(D_1 = \\{(x_1, y_1),(x_4, y_4),(x_3, y_3),(x_5, y_5),(x_5, y_5)\\}\\)\r\nSe repetir√° esta construcci√≥n aleatoria de data-sets \\(B\\) veces, obteniendo data-sets \\(D_1, D_2, ..., D_B\\)\r\nA \\(D_1, D_2, ..., D_B\\) se les conoce como muestras bootstrap.\r\nHay a lo m√°s \\(n^n\\) muestras bootstrap\r\nLas muestras bootstrap son data-sets artificiales: estos dara-sets pudieron ocurrir pero nunca se observaron.\r\nConsid√©rese un data-set \\(D\\) con \\(n\\) observaciones \\(D = \\{(x_i,y_i):i=1,...n\\}\\)\r\nSe determinan \\(m\\) muestras bootstrap del data-set original. Sean \\(D_1, ..., D_m\\) dichas muestras bootstrap.\r\n¬øCu√°l es la probabilidad de que la primera observaci√≥n no aparezca en ninguna de las muestras bootstrap?\r\nObs√©rvese que para cualquier \\(j \\in \\{1, ..., m\\}\\)\r\n\\[\r\n\\mathbb{P}((x_1,y_1)\\notin D_j) = \\bigg(1-\\frac{1}{n}\\bigg)^n = \\bigg(\\frac{n-1}{n}\\bigg)^n \r\n\\]\r\nEntonces\r\n\\[\r\n\\mathbb{P}\\bigg((x_1,y_1)\\notin D_1 \\cup D_2 \\cup ... \\cup D_m\\bigg)\\\\\r\n=\\mathbb{P}\\bigg((x_1,y_1)\\notin D_1, (x_2,y_2)\\notin D_2, ..., (x_m,y_m)\\notin D_m\\bigg) \\\\\r\n= \\prod_{j=1}^m \\mathbb{P}\\bigg((x_1,y_1)\\notin D_j\\bigg)= \\bigg(\\mathbb{P}((x_1,y_1)\\notin D_j)\\bigg)^m\\\\\r\n = \\bigg(\\bigg(1-\\frac{1}{n}\\bigg)^n\\bigg)^m = \\bigg(1-\\frac{1}{n}\\bigg)^{nm}\\\\\r\n\\rightarrow_{n \\rightarrow \\infty} (e^{-1})^m = e^{-m} = (0.3678)^m\r\n\\]\r\n\r\nConclusi√≥n. Si se determina una muestra boostrap de un data set grande, aprox \\(\\frac{2}{3}\\) de las observaciones originales estar√°n en la muestra bootstrap y \\(\\frac{1}{3}\\) no aparecer√° en la muestra bootstrap.\r\nBagging\r\nUn √°rbol de decisi√≥n induce una varianza alta con diferentes data sets es probable que se encuentren √°rboles diferentes.\r\nComo se est√° utilizando un √∫nico √°rbol, no se obtiene un algor√≠tmo suficientemente estable.\r\n\r\nSe utiliza el concepto de bagging: se ajustan \\(B\\) √°rboles diferentes y se predice la respuesta utilizando cada √°rbol por separado. Una predicci√≥n final se obtiene ‚Äúpromediando‚Äù las B diferentes predicciones.\r\nRecordatorio: \\(\\mathbb{E}(\\bar{Y}) = \\mathbb{E}(Y), \\space Var(\\bar{Y}) = \\frac{Var(Y)}{n}\\) es decir \\(Var(\\bar{Y}) < Var(Y)\\)\r\nLa t√©cnica de bootstrap lleva a una reducci√≥n de varianza.\r\nCon el data-set original, se har√° un muestreo bootstrap y se obtendr√° un nuevo data-set \\(D_1\\)\r\nCon este data ser \\(D_1\\), se ajustar√° un √°rbol\r\n\\[\r\n\\hat{y}_1 = \\hat{f}_1(\\underline{x})\r\n\\] \\(\\hat{f}_1\\) es el √°rbol ajustado\r\n\\(\\hat{y}_1\\) es la predicci√≥n bas√°ndose en \\(\\hat{f}_1\\)\r\nObtenemos una segunda muestra bootstrap y ajustamos un √°rbol:\r\n\\[\r\n\\hat{y}_2 = \\hat{f}_2(\\underline{x})\r\n\\]\r\nContinuando con este procedimiento, se obtendr√°n \\(B\\) arboles \\(\\hat{f}_1, \\hat{f}_2, ..., \\hat{f}_B\\) y \\(B\\) predicciones \\(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_B\\)\r\nEn caso de un √°rbol de regresi√≥n, los vamos a combinar de la siguiente manera\r\n\\[\r\n\\hat{f}^{bag}(\\underline{x}):= \\frac{1}{B}\\sum_{j=1}^{B}\\hat{f}_{j}(\\underline{x})\r\n\\]\r\nEn un √°rbol de clasificaci√≥n, se combina de la siguiente manera\r\n\\[\r\n\\hat{f}^{bag}(\\underline{x})=argmax\\{\\hat{f}_{j}(\\underline{x})\\}\r\n\\]\r\nObs: Un √°rbol individual, en muestra bagged puede tener un MSE menor que el √°rbol bagged.\r\nSe incrementa el n√∫mero de √°rboles individuales para avergiuar si el modelo mejora.\r\nEn caso de que se construya un √°rbol bagged incorporando un gran n√∫mero de √°rboles individuales el MSE (de prueba) ser√° m√°s estable que usa un n√∫mero peque√±o de √°rboles individuales.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-25T21:43:12-05:00",
    "input_file": "rboles-2.utf8.md"
  },
  {
    "path": "posts/2021-05-12-rboles-de-decisin/",
    "title": "√Årboles 1",
    "description": "Parte 1 de √°rboles",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-09",
    "categories": [
      "arboles"
    ],
    "contents": "\r\n\r\nContents\r\n¬øQu√© es un √°rbol de decisi√≥n?\r\n¬øC√≥mo se determinan las subregiones?\r\n\r\nLa funci√≥n de regresi√≥n\r\nDetalles de la construcci√≥n\r\nSplits binarios\r\nReglas de paro\r\nHojas de un √°rbol de decisi√≥n\r\nSplits binarios estandarizados\r\nSplits binarios estandarizados\r\n\r\nPredicciones\r\nBanda de split para √°rboles de regresi√≥n\r\nDefinici√≥n Suma de cuadrados\r\nSplit √≥ptimo\r\n\r\n√Årboles de clasificaci√≥n\r\nMisclassification error para el caso bidimensional\r\n\r\n\r\n¬øQu√© es un √°rbol de decisi√≥n?\r\nHay una variable aleatoria respuesta que se predecir√° utilizando un conjunto de variables explicativas (no aleatorias) que se conocen como variables explicativas. Es decir que es un modelo de aprendizaje supervisado.\r\nLa idea de un √°rbol de decisi√≥n es agrupar los datos en subregiones, en los que la variabilidad en cada subregi√≥n sea relativamente baja.\r\nLas subregiones se determinan usando las variables explicativas de las observaciones.\r\nUna vez que ‚Äúse decide‚Äù la subregi√≥n a la que pertenece una observaci√≥n, una predicci√≥n ser√° m√°s ‚Äúexacta‚Äù ya que la ‚Äúvariabilidad‚Äù en esta subregi√≥n es ‚Äúpeque√±a‚Äù.\r\n¬øC√≥mo se determinan las subregiones?\r\nUn √°rbol de decisi√≥n segmenta al espacio predictor en diferentes regiones utilizando splits binarios consecutivos. Se puede usar un modelo de predicci√≥n diferente en cada una de las subregiones\r\n\r\nSe har√° distinci√≥n entre:\r\n√Årboles de regresi√≥n: Respuesta continua\r\n√Årboles de clasificaci√≥n: Respuesta categ√≥rica.\r\n\r\nEn √°rboles de regresi√≥n, la calidad de las predicciones se puede evaluar midiendo la distancia entra la respuesta predecida y la respuesta observada. \\(y-\\hat{y}\\)\r\nEn √°rboles de clasificaci√≥n, la exactitud/calidad de las predicciones se puede determinar usando el misclasification error (la proporci√≥n de observaciones mal clasificadas). El objetivo es clasificar bien tantas observaciones como sea posible.\r\nSe supondr√° que hay \\(p\\) variables explicativas \\(X_1, X_2, ..., X_p\\) que se usar√°n para predecir una variable respuesta unidimensional \\(Y\\).\r\nSea \\(\\textbf{R}\\) el espacio predictor. El vector \\((X_1, ..., X_p)\\) toma valores en el espacio predictor, i.e.¬†\\((x_1, ..., x_p) \\in \\textbf{R}\\)\r\nLa funci√≥n de regresi√≥n\r\nLa variable respuesta \\(Y\\) es una variable aleatoria y se supone que se cumple la siguiente ecuaci√≥n \\[Y = f(\\underline{X}) +\\epsilon\\] Donde \\(f(\\underline{X}) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\)\r\nLa funci√≥n de regresi√≥n \\(f\\) determina el ‚Äúefecto‚Äù (ojo: da la de causalidad) de las variables explicaticas en la variable respuesta \\(Y\\).\r\nEl t√©rmino de error \\(\\epsilon\\) es una variable aleatoria con media cero, que determina las fluctuaciones de la respuesta.\r\n\r\nConsid√©rese un data set con variables explicativas y respuesta. La idea es explicar c√≥mo se utilizan los √°rboles de decisi√≥n para estimar la funci√≥n \\(f\\). Dicha estimaci√≥n se denotar√° por \\(\\hat{f}\\).\r\nDado \\(\\underline{x}\\in\\textbf{R}\\), se estimar√° la respuesta \\(\\hat{y}\\) utilizando la funci√≥n \\(\\hat{f}\\), i.e.¬†\\[ \\hat{y} = \\hat{f}(\\underline{x})\\]\r\nLa pregunta principal a resolver ¬øCupal deber√≠a ser la respuesta predecida para una nueva observaci√≥n? Esta pregunta se resuelve en dos pasos:\r\n¬øA qu√© subregi√≥n pertenece la observaci√≥n de prueba?\r\n¬øCu√°l es la respuesta predecida en esta regi√≥n?\r\n\r\nDetalles de la construcci√≥n\r\nCuando se construye un √°rbol de decisi√≥n (ya sea de clasificaci√≥n o de regresi√≥n) hay 3 detalles a considerar:\r\nRegla de splitting √≥ptimo. ¬øC√≥mo dividir cada subregi√≥n en nuevas subregiones?\r\nRegla de paro: ¬øCu√°ndo se debe parar la divisi√≥n de una sibregi√≥n?\r\nModelo de predicci√≥n: ¬øCu√°l debe ser la respuesta predecida en cada subregi√≥n?\r\n\r\nUn √°rbol de decisi√≥n es una herramienta que proporciona un plan paso a paso de c√≥mo particionar al espacio predictor completo en subregiones a trav√©s de splits binarios consecutivos.\r\nObservaci√≥n: Existen algor√≠tmos de splitting m√°s complicados adem√°s del binario.\r\nSplits binarios\r\nSe empezar√° dividiendo al espacio predictor completo en 2 subregiones. Se llamar√° a \\(R\\) como nodo ra√≠z\r\nEntonces se dividir√° al nodo ra√≠z en 2 nuevos nodos. Estos 2 nodos deben ser dos subconjuntos disjuntos del nodo ra√≠z.\r\nSean \\(R_{00}\\) y \\(R_{01}\\) los nuevos subconjuntos que conforman al nodo ra√≠z, i.e.¬†\\[R = R_{00} \\cup R_{01}, R_{00} \\cap R_{01} = \\emptyset\\]\r\nSea \\(\\xi_0 := (R_{00}, R_{01})\\). Se dice que \\(\\xi_0\\) es un split bianrio de \\(R\\)\r\n\r\nNotaci√≥n: El primer d√≠gito en la notaci√≥n \\(R_{00}\\) (i.e.¬†‚Äú0‚Äù) denota la ‚Äúgeneraci√≥n‚Äù del nodo y el segundo d√≠gito (‚Äú0‚Äù √≥ ‚Äú1‚Äù) indica si el correspondiente subespacio es hijo izquierdo (\\(R_{00}\\)) √≥ hijo derecho (\\(R_{01}\\)) del nodo madre.\r\nEl espacio predictor \\(\\textbf{R}\\) es la generaci√≥n 0 y los dos nuevos espacios son la generaci√≥n 1.\r\nEl siguiente paso es dividir a ambos subconjuntos \\(R_{00}\\) y \\(R_{01}\\) y as√≠ sucesivamente PERO algunos nodos hijos no se dividen.\r\n\r\nSup√≥ngase que se tiene un nodo \\(R_t\\) en el √°rbol de decisi√≥n y que dicho nodo es de la generaci√≥n k, ie \\(t \\in \\{0, 1\\}^{k+1}\\) (i.e.¬†una sucesi√≥n de \\(0'^s\\) y \\(1'^s\\)).\r\nPor ejemplo, si la generaci√≥n es \\(k=4\\), \\(t\\) puede ser 00101, √≥ 00100, √≥ ‚Ä¶\r\nSe puede separar/dividir este nodo en hijo izquierdo \\(R_{t0}\\) y en hijo derecho \\(R_{t1}\\), usando el split binario \\(\\xi_t = (R_{t0}, R_{t1})\\), donde:\r\n\\(R_t = R_{t0} \\cup R_{t1}\\)\r\n\\(R_{t0} \\cap R_{t1} = \\emptyset\\)\r\n\r\nNotaci√≥n: \\(\\mathbb{T} = \\{t : R_t \\text{ es un nodo del √°rbol de decis√≥n}\\}\\)\r\n¬øCu√°ndo se detiene el splitting? Una regla de paro (stoping rule) determina si un noo dado se divide o no.\r\nReglas de paro\r\nEl objetivo es generar subregiones del espacio predictor completo tal que la variabilidad entre las respuestas en cada subregi√≥n sea suficientemente peque√±a para crear predicciones ‚Äúextra√±as‚Äù.\r\nEjemplo: ‚ÄúParar si el crecimiento en la variabilidad no es suficientemente significativa‚Äù.\r\nEjemplo: ‚ÄúParar si alguno de los nodos hijos (o el mismo padre) tiene pocos elementos‚Äù.\r\n\r\nHojas de un √°rbol de decisi√≥n\r\nCuando se aplica una regla de paro, se obtiene un conjunto de notos finales (nodos sin hijos).\r\nSe llama a estos nodos finales hojas. Los otros nodos se conocen como nodos internos.\r\nSe define al conjunto \\(\\tau\\) de la siguiente forma:\r\nEl nodo \\(R_t\\) es hoja si y s√≥lo si \\(t \\in \\tau\\)\r\n\\(\\tau\\) contiene todos los √≠ndices \\(t\\) tales que \\(R_t\\) es una hoja. Claramente \\(\\tau \\in \\mathbb{T}\\)\r\nPara hacer crecer un √°rbol de decisi√≥n, se aplica una sucesi√≥n finita de splits binarios. Por lo tanto \\(\\tau\\) contiene un n√∫mero finito de hojas.\r\nSe supondr√° que \\(|\\tau| = m\\). Para simplificar la notaci√≥n, se denotar√°n a las hojas por: \\(R_1, ..., R_m\\)\r\nSplits binarios estandarizados\r\nConsid√©rese un nodo \\(R_t\\) de la generaci√≥n \\(k\\)\r\nHay muchas maneras de dividir a este espacio en 2 nodos hijos nuevos (de la generaci√≥n \\(k+1\\))\r\nEl objetivo es buscar la ‚Äúmejor‚Äù divisi√≥n de un nodo dado. Pero dividir un conjunto en dos nuevos subconjuntos se puede volver muy complicado (El problema de optimizaci√≥n puede requerir muchos recursos y no ser√° factible)\r\n\r\nSe buscar√° el mejor split en la clase de los splits binarios estandarizados (computacionalmente factible y relativamente eficiente).\r\nSplits binarios estandarizados\r\nDefinici√≥n: Un split binario estandarizado \\(\\xi_t\\) de \\(R_t\\) divide a solo una dimensi√≥n predictora \\(l \\in \\{1, ..., p\\}\\) (\\(l\\) fija) en dos partes\r\n\r\nConsid√©rese el nodo \\(R_t\\) en un √°rbol de decisi√≥n que se dividir√° en 2 nuevos nodos hijos usando un split binario estandarizado.\r\nSup√≥ngase que la variable \\(x_l\\) es cuantitativa (donde \\(l\\) es la dimensi√≥n con la que se decidi√≥ hacer el split)\r\nEntonces el split binario estandarizado del conjunto \\(R_t\\) divide al espacio en 2 partes usando la constante \\(c \\in \\mathbb{R}\\). \\[(x_1, ...,. x_p) \\in R_{t0} \\iff x_l \\leq c\\]\r\n\\[(x_1, ...,. x_p) \\in R_{t1} \\iff x_l > c\\]\r\nSup√≥ngase que la variable \\(x_l\\) es categ√≥rica. En este caso, \\(x_l = c\\) se interpreta como ‚Äú\\(x_l\\) pertenece a la clase \\(c\\)‚Äù. Un split binario estandarizado para una variable categ√≥rica \\(x_l\\) se define de la siguiente forma: \\[(x_1, ...,. x_p) \\in R_{t0} \\iff x_l \\in C\\] donde \\(C\\) es un subconjunto de todas las posibles categor√≠as que \\(x_l\\) puede tomar.\r\nEjemplo\r\n\\[x_l \\in \\{\\text{\"mexicano\", \"estadounidence\", \"canadiense\"}\\}\\] \\[C = \\{\\text{\"mexicano\", \"canadiense\"}\\}\\]\r\nTe manda al nodo derecho si eres mexicano o canadiense.\r\n\r\n\r\n\\[ R_1 \\cup R_2 \\cup ... \\cup R_m =  R \\] \\[ R_1 \\cap R_2 \\cap ... \\cap R_m =  \\emptyset \\]\r\n\\[\r\ny = f(x) = \\sum_{i=1}^{m} \\lambda_i 1_{(\\underline{x} \\in R_i)} = \\begin{cases}\r\n \\lambda_1 &\\text{ si } \\underline{x} \\in R_1\\\\         \r\n \\lambda_2 &\\text{  si  } \\underline{x} \\in R_2\\\\\r\n...\\\\\r\n\\lambda_m &\\text{ si } \\underline{x} \\in R_m        \r\n\\end{cases}\r\n\\]\r\nPredicciones\r\nSup√≥ngase que se tienen \\(p\\) variables explicativas \\(X_1, X_2, ..., X_p\\) y un √°rbol de decisi√≥n que particiona al espacio predictor en hojas \\(R_1, R_2, ..., R_m\\)\r\nConsid√©rese un vector \\(\\underline{x} \\in R\\). La respuesta \\(y = f(x)\\) se determina de la siguiente forma \\[f(x) = \\sum_{i=1}^{m} \\lambda_i 1_{(\\underline{x} \\in R_i)}\\]\r\ndonde se predice la respuesta usando una constante diferente \\(\\lambda_i\\) en cada hoja \\(R_i\\), es decir, \\(\\lambda_i\\) predice a \\(y\\), en la hoja \\(R_i\\)\r\n\r\nBanda de split para √°rboles de regresi√≥n\r\nSup√≥ngase que las variables respuesta y explicativas son cuantitativas.\r\nHay \\(p\\) variables explicativas \\(x_1, x_2, ..., x_p\\) y se tiene que decidir cu√°l de √©stas usar para dividir el espacio predictor.\r\nAdem√°s, una vez que se decidi√≥ que variable explicativa usar para dividir el espacio, se tiene que decidir d√≥nde dividir √©sta variable, es decir, encontrar el punto de corte \\(c\\)\r\nPor lo tanto, seleccionar un split √≥ptimo se reduce a determinar la variable explicativa \\(x_l\\) y una constante \\(c\\)\r\n\\[ R_{t0}  = \\{\\underline{x} \\in R_t : x_l \\leq c\\}\\]\r\n\\[ R_{t1}  = \\{\\underline{x} \\in R_t : x_l > c\\}\\]\r\nAdem√°s sup√≥ngase que el valor predecido dentro de la regi√≥n \\(R_{t0}\\) es \\(\\lambda_0\\) y dentro de la regi√≥n \\(R_{t1}\\) es \\(\\lambda_1\\)\r\n\\[\r\nf(x)= \\begin{cases}\r\n \\lambda_1 &\\text{ si } \\underline{x} \\in R_{t0}\\\\         \r\n \\lambda_2 &\\text{  si  } \\underline{x} \\in R_{t1}\r\n\\end{cases}\r\n\\]\r\nDefinici√≥n Suma de cuadrados\r\n\\[\r\nSS(l,c;\\lambda_0, \\lambda_1) = \\sum_{\\underline{x}_i \\in R_{t0}} (y_i - \\lambda_0)^2 + \\sum_{\\underline{x}_i \\in R_{t1}} (y_i - \\lambda_1)^2 \r\n\\]\r\nSplit √≥ptimo\r\nEl mejor split se define como el split que minimiza la suma de los cuadrados. Por lo tanto, se busca resolver el siguiente problema de minimizaci√≥n\r\n\\[ \r\n\\min_{l,c} \\bigg\\{ \\min_{\\lambda_0} \\sum_{\\underline{x}_i \\in R_{t0}} (y_i - \\lambda_0)^2 + \\min_{\\lambda_1} \\sum_{\\underline{x}_i \\in R_{t1}} (y_i - \\lambda_1)^2 \\bigg\\}\r\n\\]\r\nLos dos problemas de optimizaci√≥n ‚Äúinternos‚Äù se resuelven f√°cilmente:\r\n\\[\r\n\\hat{\\lambda_0} = \\frac{1}{n_{t0}} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  = \\frac{1}{|R_{t0}|} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  \r\n\\]\r\n\\[\r\n\\hat{\\lambda_0} = \\frac{1}{n_{t0}} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  = \\frac{1}{|R_{t0}|} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  \r\n\\]\r\n\r\nSup√≥ngase que una observaci√≥n \\(\\underline{x} \\in R_{t0}\\) ¬øCu√°l deber√≠a ser la predicci√≥n de \\(\\hat{y}\\)? \\(\\hat{y} := \\hat{\\lambda_0}\\)\r\nEn general la minimizaci√≥n exterior se hace num√©ricamente.\r\nConsid√©rese un nodo \\(R_t\\) que tiene \\(n\\) elementos. Sean \\(y_1, ..., y_n\\) las respuestas y \\(\\underline{x}_1, ..., \\underline{x}_n\\) las correspondientes variables explicativa.\r\nLa suma de los cuadrados, de este nodo, se define como:\r\n\\[\r\nSS(y) := \\sum_{i=1}^n (y_i - \\bar{y})^2\r\n\\]\r\n\r\nSup√≥ngase que se separa a este nodo en un hijo izquierdo y derecho minimizando la suma de cuadrados. La correspondiente suma de cuadrados se denota por \\(SS(\\hat{\\lambda_0}, \\hat{\\lambda_1})\\)\r\nEntonces\r\n\\[SS(\\hat{\\lambda_0}, \\hat{\\lambda_1}) \\leq SS(\\bar{y})\\]\r\nEs decir, \\(SS(\\hat{\\lambda_0}, \\hat{\\lambda_1})\\) est√° acotado por arriba por \\(SS(\\bar{y})\\)\r\n√Årboles de clasificaci√≥n\r\nSe tiene un problema de clasificaci√≥n, cada variable \\(y_i\\) denota cierta clase\r\nSup√≥ngase que hay \\(K\\) clases y que est√°n etiquetadas por los n√∫meros \\(1, 2, ..., K\\)\r\n\r\nConsid√©rese un √°rbol de decisi√≥n con \\(m\\) hojas, denotadas por \\(R_1, R_2, ..., R_m\\)\r\n\\(n_j\\): n√∫mero de observaciones que caen en la hoja \\(R_j\\)\r\n\\(n = n_1 + n_2 + ... + n_m\\)\r\n\r\nSe debe predecir a qu√© clase pertenece un vector predictor \\(\\underline{x}_i\\),\r\n¬øQui√©n debe ser \\(f(\\underline{x}_i)\\) si sabemos que \\(\\underline{x}_i \\in R_i\\)?\r\nUn √°rbol de clasificaci√≥n es adecuado, si es capaz de asignar la clase correcta a la mayor√≠a de las observaciones.\r\nEn cada hoja del √°rbol, se tiene que decidir qu√© clase asignar\r\nLa predicci√≥n en cierta hoja corresponde a la clase que m√°s se repite.\r\nEsta metodolog√≠a se reduce a minimizar la tasa de misclassification, i.e.¬†minimizar la probabilidad de que una observaci√≥n seleccionada aleatoriamente de esta hoja est√© mal clasificada.\r\nLa mejor situaci√≥n ocurre cuando todas las observaciones de una hoja pertenecen a la misma clase (no hay duda cuando se asigna la predicci√≥n en esta hoja y todas las observaciones en esta hoja est√°n correctamente clasificadas.)\r\nnodo puro: nodo en el que todas las observaciones pertenecen a la misma clase.\r\nSup√≥ngase que se construy√≥ un √°rbol de clasificaci√≥n con m hojas.\r\nLa mejor situaci√≥n ocurre cuando todas las m hojas del √°rbol son nodos puros (se puede clasificar a todas las observaciones de entrenamiento correctamente, usando las m hojas)\r\nCuando se construye un √°rbol de clasificaci√≥n, se separa cada nodo en 2 nodos hijos. Se buscan splits que generen nodos hijos que sean ‚Äútan puros como sea posible‚Äù.\r\nSe necesita cuantificar el grado de impureza de un nodo.\r\n\\(P_j\\) cuantifica el grado de impureza del nodo de la hoja \\(R_j\\)\r\nSi la hoja \\(R_l\\) es pura, entonces \\(P_l = 0\\)\r\nMientras m√°s grande sea \\(P_j\\), ‚Äúm√°s impura‚Äù ser√° la hoja \\(R_j\\)\r\n\\(P_j\\) grande \\(\\rightarrow\\) \\(R_j\\) muy impura\r\nSe define la frecuencia de la clase \\(k\\) en la hoja \\(j\\) denotada por \\(\\hat{P}_{jk}\\) donde \\(j\\) representa la hoja y \\(k\\) representa la clase como \\[\\hat{P}_{jk} = \\frac{1}{n_j}\\sum_{\\underline{x}_i \\in R_j}1_{(y_i = k)} \\]\r\n\\(\\hat{P}_{jk}\\) se puede interpretar como la probabilidad emp√≠rica de que una observaci√≥n caiga en la clase \\(k\\), dado que la observaci√≥n pertenece a la hoja \\(R_j\\).\r\nSe considerar√°n 3 medidas de impureza de un nodo:\r\nMisclassification error\r\n√≠ndice de Gini\r\nEntrop√≠a cruzada\r\n\r\nDefinici√≥n. Se define el misclassification error de una hoja \\(R_j\\) como\r\n\\[\r\nE_j := 1- \\max_{k}\\{\\hat{P}_{jk}\\}\r\n\\]\r\nObservaci√≥n:\r\n\\[\r\nE_j = 0 \\iff 1=\\max_{k}\\{\\hat{P}_{jk}\\} \\iff \\max\\{\\hat{P}_{j1}, \\hat{P}_{j2}, ...,\\hat{P}_{jk}\\} \\rightarrow \\exists l \\text{  tal que } \r\n\\] \\[\r\n\\hat{P}_{jl} = 1 \\text{ lo cual implica que los elementos de la hoja }j\\text{ son de la clase } l\r\n\\]\r\n\\(E_j = 0 \\rightarrow\\) Todos los \\(y_i'^{s}\\) en la hoja \\(R_j\\) son de una clase \\(l\\), i.e.¬†la hoja es pura.\r\nUna hoja \\(R_j\\) que es pura, tiene un misclassification error \\(E_j = 0\\)\r\nMientras m√°s grande sea \\(E_j\\), m√°s alta ser√° la impureza de \\(R_j\\)\r\n\\[\r\nE_j \\uparrow \\space \\rightarrow \\space \\text{Pureza de }R_j \\downarrow\r\n\\]\r\nMientras m√°s alto sea el grado de impureza, m√°s dif√≠cil ser√° predecir la clase correctamente en esa hoja particular.\r\nMisclassification error para el caso bidimensional\r\nPara el nodo \\(R_j\\) de un √°rbol de clasificaci√≥n, sup√≥ngase que hay 2 clases para respuesta: 0 √≥ 1.\r\n\\[\r\n\\hat{P} := \\hat{P}_{j0}, \\space 1-\\hat{P} = \\hat{P}_{j1} \\\\\r\nE = 1-\\max\\{p, 1-p\\}\r\n\\]\r\nLa aplicaci√≥n \\(p \\mapsto E\\) no es diferenciable y tiene un m√°ximo en \\(p = \\frac{1}{2}\\)\r\n\r\nDefinici√≥n. Se define el √≠ndice de Gini de la hoja \\(R_j\\) como\r\n\\[\r\nG_j := \\sum_{l = 1}^{k}\\hat{P}_{jl}(1-\\hat{P}_{jl})\r\n\\]\r\nDefinici√≥n. Se define la cross-entropy de la hoja \\(R_j\\) como\r\n\\[\r\nD_j := -\\sum_{l = 1}^{k}\\hat{P}_{jl}\\log(\\hat{P}_{jl})\r\n\\]\r\nPara un nodo \\(R_j\\) de un √°rbol de clasificaci√≥n, sup√≥ngase que hay 2 clases para la respuesta (0 √≥ 1).\r\n\\[\\hat{P}:=\\hat{P}_{j0} \\space 1-\\hat{P}=\\hat{P}_{j1}\\\\ G = 2p(1-p) = 2p-2p^2\\]\r\nLa aplicaci√≥n \\(p \\mapsto G\\) s√≠ es diferenciable y tiene un m√°ximo en \\(p = \\frac{1}{2}\\)\r\nObservaci√≥n:\r\n\r\nSi \\(p=1\\), entonces \\(G = 0\\) y todas las observaciones pertenecen a la clase 0\r\nSi \\(p=0\\), entonces \\(G = 1\\) y todas las observaciones pertenecen a la clase 1\r\n\\[\r\nD = -[p\\log(p) + (1-p)\\log(1-p)] \\space \\text{cross entropy}\r\n\\]\r\nLa aplicaci√≥n \\(p \\mapsto D\\) s√≠ es diferenciable y tiene un m√°ximo en \\(p = \\frac{1}{2}\\)\r\nDemostraci√≥n:\r\n\\[\r\nD¬¥(p) = -[\\log(p)-\\log(1-p)] \\rightarrow D¬¥(p) = 0 \\iff p = \\frac{1}{2} \\\\\r\nD¬¥¬¥(p) = -\\bigg[\\frac{1}{p}+\\frac{1}{1-p}\\bigg] \\rightarrow D¬¥¬¥(\\frac{1}{2}) = -4 < 0_{._\\Box}\r\n\\]\r\nTanto en el √≠ndice de Gini y el cross-entropy, la impureza se maximiza en \\(p=\\frac{1}{2}\\) y entonces la mitad de las hojas pertenecen a la clase 1 y la otra mitad a la clase 0.\r\n\r\n",
    "preview": "posts/2021-05-12-rboles-de-decisin/images/arboles1_3.png",
    "last_modified": "2021-05-15T00:08:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "¬øDe qu√© trata el blog?",
    "description": "¬°La informaci√≥n aqu√≠ presentada es para consultar teor√≠a de Ciencia de Datos que adem√°s se implementa con el programa R!",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-08",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-15T00:10:04-05:00",
    "input_file": {}
  }
]
