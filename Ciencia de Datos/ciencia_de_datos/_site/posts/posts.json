[
  {
    "path": "posts/2021-05-12-rboles-de-decisin/",
    "title": "Árboles de decisión",
    "description": "Parte 1",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-12",
    "categories": [
      "árboles"
    ],
    "contents": "\r\n\r\nContents\r\n¿Qué es un árbol de decisión?\r\n¿Cómo se determinan las subregiones?\r\n\r\nLa función de regresión\r\nDetalles de la construcción\r\nSplits binarios\r\nReglas de paro\r\nHojas de un árbol de decisión\r\nSplits binarios estandarizados\r\nSplits binarios estandarizados\r\n\r\nPredicciones\r\nBanda de split para árboles de regresión\r\nDefinición Suma de cuadrados\r\nSplit óptimo\r\n\r\nÁrboles de clasificación\r\nMisclassification error para el caso bidimensional\r\n\r\n\r\n¿Qué es un árbol de decisión?\r\nHay una variable aleatoria respuesta que se predecirá utilizando un conjunto de variables explicativas (no aleatorias) que se conocen como variables explicativas. Es decir que es un modelo de aprendizaje supervisado.\r\nLa idea de un árbol de decisión es agrupar los datos en subregiones, en los que la variabilidad en cada subregión sea relativamente baja.\r\nLas subregiones se determinan usando las variables explicativas de las observaciones.\r\nUna vez que “se decide” la subregión a la que pertenece una observación, una predicción será más “exacta” ya que la “variabilidad” en esta subregión es “pequeña”.\r\n¿Cómo se determinan las subregiones?\r\nUn árbol de decisión segmenta al espacio predictor en diferentes regiones utilizando splits binarios consecutivos. Se puede usar un modelo de predicción diferente en cada una de las subregiones\r\n\r\nSe hará distinción entre:\r\nÁrboles de regresión: Respuesta continua\r\nÁrboles de clasificación: Respuesta categórica.\r\n\r\nEn árboles de regresión, la calidad de las predicciones se puede evaluar midiendo la distancia entra la respuesta predecida y la respuesta observada. \\(y-\\hat{y}\\)\r\nEn árboles de clasificación, la exactitud/calidad de las predicciones se puede determinar usando el misclasification error (la proporción de observaciones mal clasificadas). El objetivo es clasificar bien tantas observaciones como sea posible.\r\nSe supondrá que hay \\(p\\) variables explicativas \\(X_1, X_2, ..., X_p\\) que se usarán para predecir una variable respuesta unidimensional \\(Y\\).\r\nSea \\(\\textbf{R}\\) el espacio predictor. El vector \\((X_1, ..., X_p)\\) toma valores en el espacio predictor, i.e. \\((x_1, ..., x_p) \\in \\textbf{R}\\)\r\nLa función de regresión\r\nLa variable respuesta \\(Y\\) es una variable aleatoria y se supone que se cumple la siguiente ecuación \\[Y = f(\\underline{X}) +\\epsilon\\] Donde \\(f(\\underline{X}) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\)\r\nLa función de regresión \\(f\\) determina el “efecto” (ojo: da la de causalidad) de las variables explicaticas en la variable respuesta \\(Y\\).\r\nEl término de error \\(\\epsilon\\) es una variable aleatoria con media cero, que determina las fluctuaciones de la respuesta.\r\n\r\nConsidérese un data set con variables explicativas y respuesta. La idea es explicar cómo se utilizan los árboles de decisión para estimar la función \\(f\\). Dicha estimación se denotará por \\(\\hat{f}\\).\r\nDado \\(\\underline{x}\\in\\textbf{R}\\), se estimará la respuesta \\(\\hat{y}\\) utilizando la función \\(\\hat{f}\\), i.e. \\[ \\hat{y} = \\hat{f}(\\underline{x})\\]\r\nLa pregunta principal a resolver ¿Cupal debería ser la respuesta predecida para una nueva observación? Esta pregunta se resuelve en dos pasos:\r\n¿A qué subregión pertenece la observación de prueba?\r\n¿Cuál es la respuesta predecida en esta región?\r\n\r\nDetalles de la construcción\r\nCuando se construye un árbol de decisión (ya sea de clasificación o de regresión) hay 3 detalles a considerar:\r\nRegla de splitting óptimo. ¿Cómo dividir cada subregión en nuevas subregiones?\r\nRegla de paro: ¿Cuándo se debe parar la división de una sibregión?\r\nModelo de predicción: ¿Cuál debe ser la respuesta predecida en cada subregión?\r\n\r\nUn árbol de decisión es una herramienta que proporciona un plan paso a paso de cómo particionar al espacio predictor completo en subregiones a través de splits binarios consecutivos.\r\nObservación: Existen algorítmos de splitting más complicados además del binario.\r\nSplits binarios\r\nSe empezará dividiendo al espacio predictor completo en 2 subregiones. Se llamará a \\(R\\) como nodo raíz\r\nEntonces se dividirá al nodo raíz en 2 nuevos nodos. Estos 2 nodos deben ser dos subconjuntos disjuntos del nodo raíz.\r\nSean \\(R_{00}\\) y \\(R_{01}\\) los nuevos subconjuntos que conforman al nodo raíz, i.e. \\[R = R_{00} \\cup R_{01}, R_{00} \\cap R_{01} = \\emptyset\\]\r\nSea \\(\\xi_0 := (R_{00}, R_{01})\\). Se dice que \\(\\xi_0\\) es un split bianrio de \\(R\\)\r\n\r\nNotación: El primer dígito en la notación \\(R_{00}\\) (i.e. “0”) denota la “generación” del nodo y el segundo dígito (“0” ó “1”) indica si el correspondiente subespacio es hijo izquierdo (\\(R_{00}\\)) ó hijo derecho (\\(R_{01}\\)) del nodo madre.\r\nEl espacio predictor \\(\\textbf{R}\\) es la generación 0 y los dos nuevos espacios son la generación 1.\r\nEl siguiente paso es dividir a ambos subconjuntos \\(R_{00}\\) y \\(R_{01}\\) y así sucesivamente PERO algunos nodos hijos no se dividen.\r\n\r\nSupóngase que se tiene un nodo \\(R_t\\) en el árbol de decisión y que dicho nodo es de la generación k, ie \\(t \\in \\{0, 1\\}^{k+1}\\) (i.e. una sucesión de \\(0'^s\\) y \\(1'^s\\)).\r\nPor ejemplo, si la generación es \\(k=4\\), \\(t\\) puede ser 00101, ó 00100, ó …\r\nSe puede separar/dividir este nodo en hijo izquierdo \\(R_{t0}\\) y en hijo derecho \\(R_{t1}\\), usando el split binario \\(\\xi_t = (R_{t0}, R_{t1})\\), donde:\r\n\\(R_t = R_{t0} \\cup R_{t1}\\)\r\n\\(R_{t0} \\cap R_{t1} = \\emptyset\\)\r\n\r\nNotación: \\(\\mathbb{T} = \\{t : R_t \\text{ es un nodo del árbol de decisón}\\}\\)\r\n¿Cuándo se detiene el splitting? Una regla de paro (stoping rule) determina si un noo dado se divide o no.\r\nReglas de paro\r\nEl objetivo es generar subregiones del espacio predictor completo tal que la variabilidad entre las respuestas en cada subregión sea suficientemente pequeña para crear predicciones “extrañas”.\r\nEjemplo: “Parar si el crecimiento en la variabilidad no es suficientemente significativa”.\r\nEjemplo: “Parar si alguno de los nodos hijos (o el mismo padre) tiene pocos elementos”.\r\n\r\nHojas de un árbol de decisión\r\nCuando se aplica una regla de paro, se obtiene un conjunto de notos finales (nodos sin hijos).\r\nSe llama a estos nodos finales hojas. Los otros nodos se conocen como nodos internos.\r\nSe define al conjunto \\(\\tau\\) de la siguiente forma:\r\nEl nodo \\(R_t\\) es hoja si y sólo si \\(t \\in \\tau\\)\r\n\\(\\tau\\) contiene todos los índices \\(t\\) tales que \\(R_t\\) es una hoja. Claramente \\(\\tau \\in \\mathbb{T}\\)\r\nPara hacer crecer un árbol de decisión, se aplica una sucesión finita de splits binarios. Por lo tanto \\(\\tau\\) contiene un número finito de hojas.\r\nSe supondrá que \\(|\\tau| = m\\). Para simplificar la notación, se denotarán a las hojas por: \\(R_1, ..., R_m\\)\r\nSplits binarios estandarizados\r\nConsidérese un nodo \\(R_t\\) de la generación \\(k\\)\r\nHay muchas maneras de dividir a este espacio en 2 nodos hijos nuevos (de la generación \\(k+1\\))\r\nEl objetivo es buscar la “mejor” división de un nodo dado. Pero dividir un conjunto en dos nuevos subconjuntos se puede volver muy complicado (El problema de optimización puede requerir muchos recursos y no será factible)\r\n\r\nSe buscará el mejor split en la clase de los splits binarios estandarizados (computacionalmente factible y relativamente eficiente).\r\nSplits binarios estandarizados\r\nDefinición: Un split binario estandarizado \\(\\xi_t\\) de \\(R_t\\) divide a solo una dimensión predictora \\(l \\in \\{1, ..., p\\}\\) (\\(l\\) fija) en dos partes\r\n\r\nConsidérese el nodo \\(R_t\\) en un árbol de decisión que se dividirá en 2 nuevos nodos hijos usando un split binario estandarizado.\r\nSupóngase que la variable \\(x_l\\) es cuantitativa (donde \\(l\\) es la dimensión con la que se decidió hacer el split)\r\nEntonces el split binario estandarizado del conjunto \\(R_t\\) divide al espacio en 2 partes usando la constante \\(c \\in \\mathbb{R}\\). \\[(x_1, ...,. x_p) \\in R_{t0} \\iff x_l \\leq c\\]\r\n\\[(x_1, ...,. x_p) \\in R_{t1} \\iff x_l > c\\]\r\nSupóngase que la variable \\(x_l\\) es categórica. En este caso, \\(x_l = c\\) se interpreta como “\\(x_l\\) pertenece a la clase \\(c\\)”. Un split binario estandarizado para una variable categórica \\(x_l\\) se define de la siguiente forma: \\[(x_1, ...,. x_p) \\in R_{t0} \\iff x_l \\in C\\] donde \\(C\\) es un subconjunto de todas las posibles categorías que \\(x_l\\) puede tomar.\r\nEjemplo\r\n\\[x_l \\in \\{\\text{\"mexicano\", \"estadounidence\", \"canadiense\"}\\}\\] \\[C = \\{\\text{\"mexicano\", \"canadiense\"}\\}\\]\r\nTe manda al nodo derecho si eres mexicano o canadiense.\r\n\r\n\r\n\\[ R_1 \\cup R_2 \\cup ... \\cup R_m =  R \\] \\[ R_1 \\cap R_2 \\cap ... \\cap R_m =  \\emptyset \\]\r\n\\[\r\ny = f(x) = \\sum_{i=1}^{m} \\lambda_i 1_{(\\underline{x} \\in R_i)} = \\begin{cases}\r\n \\lambda_1 &\\text{ si } \\underline{x} \\in R_1\\\\         \r\n \\lambda_2 &\\text{  si  } \\underline{x} \\in R_2\\\\\r\n...\\\\\r\n\\lambda_m &\\text{ si } \\underline{x} \\in R_m        \r\n\\end{cases}\r\n\\]\r\nPredicciones\r\nSupóngase que se tienen \\(p\\) variables explicativas \\(X_1, X_2, ..., X_p\\) y un árbol de decisión que particiona al espacio predictor en hojas \\(R_1, R_2, ..., R_m\\)\r\nConsidérese un vector \\(\\underline{x} \\in R\\). La respuesta \\(y = f(x)\\) se determina de la siguiente forma \\[f(x) = \\sum_{i=1}^{m} \\lambda_i 1_{(\\underline{x} \\in R_i)}\\]\r\ndonde se predice la respuesta usando una constante diferente \\(\\lambda_i\\) en cada hoja \\(R_i\\), es decir, \\(\\lambda_i\\) predice a \\(y\\), en la hoja \\(R_i\\)\r\n\r\nBanda de split para árboles de regresión\r\nSupóngase que las variables respuesta y explicativas son cuantitativas.\r\nHay \\(p\\) variables explicativas \\(x_1, x_2, ..., x_p\\) y se tiene que decidir cuál de éstas usar para dividir el espacio predictor.\r\nAdemás, una vez que se decidió que variable explicativa usar para dividir el espacio, se tiene que decidir dónde dividir ésta variable, es decir, encontrar el punto de corte \\(c\\)\r\nPor lo tanto, seleccionar un split óptimo se reduce a determinar la variable explicativa \\(x_l\\) y una constante \\(c\\)\r\n\\[ R_{t0}  = \\{\\underline{x} \\in R_t : x_l \\leq c\\}\\]\r\n\\[ R_{t1}  = \\{\\underline{x} \\in R_t : x_l > c\\}\\]\r\nAdemás supóngase que el valor predecido dentro de la región \\(R_{t0}\\) es \\(\\lambda_0\\) y dentro de la región \\(R_{t1}\\) es \\(\\lambda_1\\)\r\n\\[\r\nf(x)= \\begin{cases}\r\n \\lambda_1 &\\text{ si } \\underline{x} \\in R_{t0}\\\\         \r\n \\lambda_2 &\\text{  si  } \\underline{x} \\in R_{t1}\r\n\\end{cases}\r\n\\]\r\nDefinición Suma de cuadrados\r\n\\[\r\nSS(l,c;\\lambda_0, \\lambda_1) = \\sum_{\\underline{x}_i \\in R_{t0}} (y_i - \\lambda_0)^2 + \\sum_{\\underline{x}_i \\in R_{t1}} (y_i - \\lambda_1)^2 \r\n\\]\r\nSplit óptimo\r\nEl mejor split se define como el split que minimiza la suma de los cuadrados. Por lo tanto, se busca resolver el siguiente problema de minimización\r\n\\[ \r\n\\min_{l,c} \\bigg\\{ \\min_{\\lambda_0} \\sum_{\\underline{x}_i \\in R_{t0}} (y_i - \\lambda_0)^2 + \\min_{\\lambda_1} \\sum_{\\underline{x}_i \\in R_{t1}} (y_i - \\lambda_1)^2 \\bigg\\}\r\n\\]\r\nLos dos problemas de optimización “internos” se resuelven fácilmente:\r\n\\[\r\n\\hat{\\lambda_0} = \\frac{1}{n_{t0}} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  = \\frac{1}{|R_{t0}|} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  \r\n\\]\r\n\\[\r\n\\hat{\\lambda_0} = \\frac{1}{n_{t0}} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  = \\frac{1}{|R_{t0}|} \\sum_{\\underline{x}_i \\in R_{t0}} y_i  \r\n\\]\r\n\r\nSupóngase que una observación \\(\\underline{x} \\in R_{t0}\\) ¿Cuál debería ser la predicción de \\(\\hat{y}\\)? \\(\\hat{y} := \\hat{\\lambda_0}\\)\r\nEn general la minimización exterior se hace numéricamente.\r\nConsidérese un nodo \\(R_t\\) que tiene \\(n\\) elementos. Sean \\(y_1, ..., y_n\\) las respuestas y \\(\\underline{x}_1, ..., \\underline{x}_n\\) las correspondientes variables explicativa.\r\nLa suma de los cuadrados, de este nodo, se define como:\r\n\\[\r\nSS(y) := \\sum_{i=1}^n (y_i - \\bar{y})^2\r\n\\]\r\n\r\nSupóngase que se separa a este nodo en un hijo izquierdo y derecho minimizando la suma de cuadrados. La correspondiente suma de cuadrados se denota por \\(SS(\\hat{\\lambda_0}, \\hat{\\lambda_1})\\)\r\nEntonces\r\n\\[SS(\\hat{\\lambda_0}, \\hat{\\lambda_1}) \\leq SS(\\bar{y})\\]\r\nEs decir, \\(SS(\\hat{\\lambda_0}, \\hat{\\lambda_1})\\) está acotado por arriba por \\(SS(\\bar{y})\\)\r\nÁrboles de clasificación\r\nSe tiene un problema de clasificación, cada variable \\(y_i\\) denota cierta clase\r\nSupóngase que hay \\(K\\) clases y que están etiquetadas por los números \\(1, 2, ..., K\\)\r\n\r\nConsidérese un árbol de decisión con \\(m\\) hojas, denotadas por \\(R_1, R_2, ..., R_m\\)\r\n\\(n_j\\): número de observaciones que caen en la hoja \\(R_j\\)\r\n\\(n = n_1 + n_2 + ... + n_m\\)\r\n\r\nSe debe predecir a qué clase pertenece un vector predictor \\(\\underline{x}_i\\),\r\n¿Quién debe ser \\(f(\\underline{x}_i)\\) si sabemos que \\(\\underline{x}_i \\in R_i\\)?\r\nUn árbol de clasificación es adecuado, si es capaz de asignar la clase correcta a la mayoría de las observaciones.\r\nEn cada hoja del árbol, se tiene que decidir qué clase asignar\r\nLa predicción en cierta hoja corresponde a la clase que más se repite.\r\nEsta metodología se reduce a minimizar la tasa de misclassification, i.e. minimizar la probabilidad de que una observación seleccionada aleatoriamente de esta hoja esté mal clasificada.\r\nLa mejor situación ocurre cuando todas las observaciones de una hoja pertenecen a la misma clase (no hay duda cuando se asigna la predicción en esta hoja y todas las observaciones en esta hoja están correctamente clasificadas.)\r\nnodo puro: nodo en el que todas las observaciones pertenecen a la misma clase.\r\nSupóngase que se construyó un árbol de clasificación con m hojas.\r\nLa mejor situación ocurre cuando todas las m hojas del árbol son nodos puros (se puede clasificar a todas las observaciones de entrenamiento correctamente, usando las m hojas)\r\nCuando se construye un árbol de clasificación, se separa cada nodo en 2 nodos hijos. Se buscan splits que generen nodos hijos que sean “tan puros como sea posible”.\r\nSe necesita cuantificar el grado de impureza de un nodo.\r\n\\(P_j\\) cuantifica el grado de impureza del nodo de la hoja \\(R_j\\)\r\nSi la hoja \\(R_l\\) es pura, entonces \\(P_l = 0\\)\r\nMientras más grande sea \\(P_j\\), “más impura” será la hoja \\(R_j\\)\r\n\\(P_j\\) grande \\(\\rightarrow\\) \\(R_j\\) muy impura\r\nSe define la frecuencia de la clase \\(k\\) en la hoja \\(j\\) denotada por \\(\\hat{P}_{jk}\\) donde \\(j\\) representa la hoja y \\(k\\) representa la clase como \\[\\hat{P}_{jk} = \\frac{1}{n_j}\\sum_{\\underline{x}_i \\in R_j}1_{(y_i = k)} \\]\r\n\\(\\hat{P}_{jk}\\) se puede interpretar como la probabilidad empírica de que una observación caiga en la clase \\(k\\), dado que la observación pertenece a la hoja \\(R_j\\).\r\nSe considerarán 3 medidas de impureza de un nodo:\r\nMisclassification error\r\níndice de Gini\r\nEntropía cruzada\r\n\r\nDefinición. Se define el misclassification error de una hoja \\(R_j\\) como\r\n\\[\r\nE_j := 1- \\max_{k}\\{\\hat{P}_{jk}\\}\r\n\\]\r\nObservación:\r\n\\[\r\nE_j = 0 \\iff 1=\\max_{k}\\{\\hat{P}_{jk}\\} \\iff \\max\\{\\hat{P}_{j1}, \\hat{P}_{j2}, ...,\\hat{P}_{jk}\\} \\rightarrow \\exists l \\text{  tal que } \r\n\\] \\[\r\n\\hat{P}_{jl} = 1 \\text{ lo cual implica que los elementos de la hoja }j\\text{ son de la clase } l\r\n\\]\r\n\\(E_j = 0 \\rightarrow\\) Todos los \\(y_i'^{s}\\) en la hoja \\(R_j\\) son de una clase \\(l\\), i.e. la hoja es pura.\r\nUna hoja \\(R_j\\) que es pura, tiene un misclassification error \\(E_j = 0\\)\r\nMientras más grande sea \\(E_j\\), más alta será la impureza de \\(R_j\\)\r\n\\[\r\nE_j \\uparrow \\space \\rightarrow \\space \\text{Pureza de }R_j \\downarrow\r\n\\]\r\nMientras más alto sea el grado de impureza, más difícil será predecir la clase correctamente en esa hoja particular.\r\nMisclassification error para el caso bidimensional\r\nPara el nodo \\(R_j\\) de un árbol de clasificación, supóngase que hay 2 clases para respuesta: 0 ó 1.\r\n\\[\r\n\\hat{P} := \\hat{P}_{j0}, \\space 1-\\hat{P} = \\hat{P}_{j1} \\\\\r\nE = 1-\\max\\{p, 1-p\\}\r\n\\]\r\nLa aplicación \\(p \\mapsto E\\) no es diferenciable y tiene un máximo en \\(p = \\frac{1}{2}\\)\r\n\r\nDefinición. Se define el índice de Gini de la hoja \\(R_j\\) como\r\n\\[\r\nG_j := \\sum_{l = 1}^{k}\\hat{P}_{jl}(1-\\hat{P}_{jl})\r\n\\]\r\nDefinición. Se define la cross-entropy de la hoja \\(R_j\\) como\r\n\\[\r\nD_j := -\\sum_{l = 1}^{k}\\hat{P}_{jl}\\log(\\hat{P}_{jl})\r\n\\]\r\nPara un nodo \\(R_j\\) de un árbol de clasificación, supóngase que hay 2 clases para la respuesta (0 ó 1).\r\n\\[\\hat{P}:=\\hat{P}_{j0} \\space 1-\\hat{P}=\\hat{P}_{j1}\\\\ G = 2p(1-p) = 2p-2p^2\\]\r\nLa aplicación \\(p \\mapsto G\\) sí es diferenciable y tiene un máximo en \\(p = \\frac{1}{2}\\)\r\nObservación:\r\n\r\nSi \\(p=1\\), entonces \\(G = 0\\) y todas las observaciones pertenecen a la clase 0\r\nSi \\(p=0\\), entonces \\(G = 1\\) y todas las observaciones pertenecen a la clase 1\r\n\\[\r\nD = -[p\\log(p) + (1-p)\\log(1-p)] \\space \\text{cross entropy}\r\n\\]\r\nLa aplicación \\(p \\mapsto D\\) sí es diferenciable y tiene un máximo en \\(p = \\frac{1}{2}\\)\r\nDemostración:\r\n\\[\r\nD´(p) = -[\\log(p)-\\log(1-p)] \\rightarrow D´(p) = 0 \\iff p = \\frac{1}{2} \\\\\r\nD´´(p) = -\\bigg[\\frac{1}{p}+\\frac{1}{1-p}\\bigg] \\rightarrow D´´(\\frac{1}{2}) = -4 < 0_{._\\Box}\r\n\\]\r\nTanto en el índice de Gini y el cross-entropy, la impureza se maximiza en \\(p=\\frac{1}{2}\\) y entonces la mitad de las hojas pertenecen a la clase 1 y la otra mitad a la clase 0.\r\n\r\n",
    "preview": "posts/2021-05-12-rboles-de-decisin/images/arboles1_3.png",
    "last_modified": "2021-05-14T01:16:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "¿De qué trata el blog?",
    "description": "¡La información aquí presentada es para consultar teoría de Ciencia de Datos que además se implementa con el programa R!",
    "author": [
      {
        "name": "Eduardo Selim M. M.",
        "url": {}
      },
      {
        "name": "Carlos A. Ar.",
        "url": {}
      }
    ],
    "date": "2021-05-12",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-12T21:47:12-05:00",
    "input_file": {}
  }
]
