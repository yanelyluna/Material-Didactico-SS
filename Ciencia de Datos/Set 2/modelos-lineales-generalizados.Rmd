---
title: "Modelos Lineales Generalizados"
author: 
    - name: "Eduardo Selim"
    - name: "Carlos A. Ar."
output: 
  html_document:
     toc: true
     toc_float:
        collapsed: false
        smooth_scroll: false
     toc_depth: 2
     number_sections: false
     theme: flatly
     highlight: tango
---


$$
\underbrace{g(y)}_{\text{Funci贸n } \\ \text{de la respuesta.}} = \beta_0 + \beta_1x_1 + ... + \beta_k x_k + \epsilon
$$

## Formulaci贸n del modelo

Hay dos componentes

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+
| Componente 1                                                                                                                                                                  | Componente 2                                                                                                                                             |
+===============================================================================================================================================================================+==========================================================================================================================================================+
| Familia de **distribuciones lineal exponencial**. La elecci贸n de una distribuci贸n particular generalmente se ayuda de la naturaleza de la variable respuesta en un escenario. | **Funci贸n link.** Ya que se especific贸 una distribuci贸n de la familia lineal exponencial, se necesita relacionar a la media de la componente sistem谩tica |
|                                                                                                                                                                               |                                                                                                                                                          |
|                                                                                                                                                                               | $$                                                                                                                                                       |
|                                                                                                                                                                               | \beta_0 + \beta_1x_1 + ... + \beta_k x_k + \epsilon                                                                                                      |
|                                                                                                                                                                               | $$                                                                                                                                                       |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+

## Definici贸n (familia lineal exponencial)

Se dice que una distribuci贸n pertenece a la [familia lineal exponencial]{.ul} si su funci贸n de densidad se puede escribir como:

$$
f(y; \theta, \phi) = \exp\bigg\{ \frac{y\theta - b(\theta)}{\phi} + S(y,\theta)\bigg\}
$$

donde

-   $\theta$: Es el par谩metro de inter茅s (generalmente de la media).

-   $\phi$: Par谩metro de escala.

-   $b(\theta)$: Funci贸n de $\theta$ (conocido)

-   $S(y, \phi)$: Es una funci贸n de $\phi$ (conocido)

Afortunadamente, muchos de los distribudores que ya conocen pertenecen a la familia exponencial lineal

|           |                   |                     |
|-----------|-------------------|---------------------|
| Bernoulli | Gamma             | Beta                |
| Poisson   | Normal            | Gamma Inversa       |
| Binomial  | Geom茅trica        | Gaussiana Invertida |
|           | Binomial Negativa |                     |

Si la distribuci贸n de $Y$ pertenece a la LED (Linear Exponencial Distribution) entonces

$$
\mu := \mathbb{E}(Y) = b'(\theta)\\
Var(Y) = \phi b''(\theta)
$$

Es 煤til explicar a la $Var(Y)$ en t茅rminos de $\mathbb{E}(Y)$ i.e. $Var(Y) = \phi \nu(\mu)$ a $\nu(\mu)$ se le conoce como **funci贸n varianza.**

Una propiedad bonita en LED es que $\nu(\mu) = b''(\theta)$

### Ejemplo:

Si $Y\sim Poisson(\lambda)$, es decir, $f_Y(y) = \frac{e^{-\lambda}\lambda^y}{y!} \bigg|^y_{\{0,1,...\}}$ 驴Y pertenece a la familia exponencial lineal? S铆

*Demostraci贸n.*

$$
f_Y(y) = \frac{e^{-\lambda}\lambda^y}{y!} = \exp\{ -\lambda + y\log(y) - \log(y!)\} = \exp\{  y\theta - e^\theta - \log(y!)\}, \\ \text{ donde } \theta = \log(\lambda)\\
\\
\therefore f_{Y} \in LED_{\Box}
$$

En el caso poisson se tiene que

+--------------------------+-------------+:-----------------------------------------------------:+
| $$                       | $$          | $$                                                    |
| \theta = \log(\lambda)\\ | \Rightarrow | \mathbb{E}(Y) = b'(\theta) = e^\theta = \lambda \\    |
| b(\theta) = e^\theta\\   | $$          | Var(Y) =\phi b''(\theta) = 1 \cdot e^\theta = \lambda |
| \phi = 1\\               |             | $$                                                    |
| S(y, \phi) = -\log(y!)   |             |                                                       |
| $$                       |             | Que es algo ya esperado                               |
+--------------------------+-------------+-------------------------------------------------------+

## Funciones link

Esta relaciona a la media de la respuesta con la combinaci贸n lineal de las variables explicativas i.e.

$$
g(\mu) = \mathbb{x}^T \beta = \beta_0 + \beta_1x_1 + ... + \beta_k x_k 
$$

Donde $(\beta_0,.... \beta_k) = \beta$ es un vector de par谩metros a estimar donde $g(\cdot)$ es mon贸tona (creciente o decreciente).

-   La elecci贸n de $g(\cdot)$ est谩 motivada por la forma funcional entre la respuesta, y las variables explicativas.

**GML:** Una LED para la respuesta & Una funci贸n link

Hay una funci贸n link especial, que reconoce como la [funci贸n link can贸nica]{.ul} que satisface la siguiente:

$$
g(\mu) = \theta
$$ donde es el par谩metro de la LED.

### Ejemplos

-   Normal: $g(\mu) = \mu$

-   Bernoulli: $g(\pi) = \log(\frac{\pi}{1-\pi})=: logit(\pi)$

-   Poisson: $g(\mu) = \log(\mu)$

-   Gamma: $g(\mu) = \frac{1}{\mu}$

-   Inversa Gaussiana: $g(\mu) = \frac{1}{\mu^2}$

**Recordatorio:** Para el caso Bernoulli

$$
\log\bigg(\frac{\pi}{1-\pi}\bigg) = \beta_0 + \beta_1x_1 +... + \beta_kx_k 
\iff
\pi = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_k x_k }}{1+e^{\beta_0 + \beta_1x_1 + ... + \beta_k x_k }}
$$

Si $\pi \geq 0.5 \rightarrow \hat{y} = 1$

Si $\pi < 0.5 \rightarrow \hat{y} = 0$

**Recordatorio:** Poisson

$$
\log(\lambda) = \beta_0 + \beta_1x_1 +... + \beta_kx_k \\
\rightarrow \lambda = \exp\{\beta_0 + \beta_1x_1 +... + \beta_kx_k \}\\
\rightarrow \hat{\lambda} = \exp\{\hat{\beta_0} + \hat{\beta_1}x_1 +... + \hat{\beta_k}x_k \}
$$

Como siempre hay que saber **驴Qu茅 nos interesa?**

1.  Estimar $\beta'^s$
2.  La variabilidad de los $\beta'^s$
3.  Bondad de ajuste: Devianza
4.  Poder de predicci贸n
5.  Construcci贸n del modelo (selecci贸n de variables explicativas).

## Estimaci贸n y predicci贸n

-   Se observan $y_1, ..., y_n$ independientes entre s铆 todas de la familia exponencial lineal pero posiblemente con diferentes valores de $\theta$ y $\phi$ i.e. $\theta_1, ..., \theta_n$ y $\phi_1, ..., \phi_n$ y por lo tanto diferentes medias.

### Estimaci贸n m谩ximo veros铆mil

Sea $L$ la funci贸n de verosimilitud. Como $f_{Y_i} \in LED$ Entonces:

$$
L(\beta) = \prod_{i=1}^n f_{Y_i}(y_i) = \prod_{i=1}^n \exp\bigg\{\frac{y_i\theta_i - b(\theta_i)}{\phi_i} + S(y_i, \phi_i)\bigg\}
$$

Adem谩s vemos que se est谩 utilizando el link can贸nico y esto nos permitir谩 escribir:

$$
L(\beta) = \prod_{i=1}^n \exp\bigg\{\frac{y_i (\mathbb{x}_i^{_T}\beta) - b(\mathbb{x}_i^{_T}\beta)}{\phi_i} + S(y_i, \phi_i)\bigg\}
$$

Como antes, preferimos obtener la log-verosimilitud.

$$
l(\beta) = \log(L(\beta)) = \sum_{i=1}^n \bigg(\frac{y_i (\mathbb{x}_i^{_T}\beta) - b(\mathbb{x}_i^{_T}\beta)}{\phi_i} + S(y_i, \phi_i)\bigg)
$$

Para encontrar el estimador m谩ximo veros铆mil,

$$
\text{Ecuaci贸n score:}\\
\boxed{\frac{\partial}{\partial \beta} l(\beta) = \underline{0}}
$$

En este caso

$$
\frac{\partial}{\partial \beta} l(\beta) = \sum_{i=1}^n \bigg(\frac{y_i \mathbb{x}_i - b'(\mathbb{x}_i^{_T}\beta)\mathbb{x}_i}{\phi_i}\bigg) = \sum_{i=1}^n \bigg(\frac{y_i \mathbb{x}_i - \mu_i\mathbb{x}_i}{\phi_i}\bigg) = \sum_{i=1}^n \bigg(\frac{y_i  - \mu_i}{\phi_i}\bigg)\mathbb{x}_i
$$

Se tiene que resolver

$$
\sum_{i=1}^n \bigg(\frac{y_i  - \mu_i}{\phi_i}\bigg)\mathbb{x}_i = \underline{0} = \begin{pmatrix}
0 \\
   \vdots \\
 0
 \end{pmatrix}
$$

**"Mala" noticias.** Esto se resuelve num茅ricamente

Afortunadamente lo hace R o Python.

[Resumiendo]{.ul}

Sup贸ngase que se desea estimar

$$
\beta = (\beta_0, ..., \beta_k) \rightarrow \hat{\beta} = (\hat{\beta}_0, ..., \hat{\beta}_k)
$$

-   **Paso 1**: Calcular el valor estimado de la "linked mean"

    $$
    g(\hat{\mu}) = \mathbb{x}^T \hat{\beta} = \hat{\beta_0} + \hat{\beta_1}x_1 + ... + \hat{\beta_k} x_k 
    $$

-   **Paso 2**: Invertir $g$, para obtener **la media** ajustada/estimada, i.e.

    $$
    \hat{\mu} = g^{-1} (\mathbb{x}^{_T}\hat{\beta})
    $$

-   **Notaci贸n.** Sea la notaci贸n $\mu$ 贸 $\hat{y}$ para denotar a la media ajustada.

### Algunas propiedades

\
$\mu$ es el estimador m谩ximo veros铆mil de $\mu = g^{-1}(\underline{x}^{_T}\beta)$

Este es:

-   Consistente

-   Asint贸ticamente insesgado

-   Asint贸ticamente gaussiano

La propiedad que hace especial a la funci贸n link can贸nica es que el estimado es insesgado.

**Ejemplo**

En el caso Bernoulli $g(\pi) = \log(\frac{\pi}{1-\pi})$ 驴Qui茅n es $g^{-1}(t)$?

Sea $z = \log(\frac{\pi}{1-\pi})$, despejemos a $\pi$

$$
e^{z} = \frac{\pi}{1-\pi} \iff e^z-\pi e^z = \pi \iff e^z = \pi(1+e^z) \iff \pi = \frac{e^z}{1+e^z} \\
\therefore g^{-1}(t) = \frac{e^t}{1+e^t} 
$$

Entonces nos recuerda a un viejo conocido 答

$$
\hat{\pi} = \frac{e^{\mathbb{x}^{_T} \hat{\beta}}}{1+e^{\mathbb{x}^{_T} \hat{\beta}}} \\
 \text{Probabilidades estimadas en el modelo de regresi贸n log铆stica}
$$

## Evaluaci贸n de la bondad de ajuste

-   El GLM ya [no]{.ul} se tiene la descomposici贸n

    $$
    TSS = RSS + RegSS
    $$

     Por lo tanto no est谩 definido el coeficiente de determinaci贸n $$R^2$$ 

-   Las medidas comunes son la devianza y la pseudo $R^2$

#### Modelo Saturado

-   El modelo saturado es uno con la misma distribuci贸n que la respuesta y funci贸n link que el GLM ajustado [pero]{.ul} con tantos par谩metros como observaciones.

    -   Esto permite maximizar la funci贸n de verosimilitud sobre una base observaci贸n por observaci贸n.

**驴Qu茅 significa esto?**

-   El modelo saturado es tal que los valores ajustados son exactamente iguales a los valores observados, i.e.

    $$
    \underbrace{\hat{\mu}_i = y_i}_{\text{bajo el modelo saturado}} \text{ para todo } i = 1,...,n
    $$

-   驴Y la justificaci贸n matem谩tica?

    $$
    \frac{\partial}{\partial \theta_i} \log(f(y_i;\theta_i,\phi_i)) = \frac{y_i - \overbrace{b'(\theta_i)}^{\hat{\mu_i}}}{\phi_i} = 0
    $$

    por lo tanto, la media ajustada $\hat{\mu}_i = b'(\hat{\theta_i})$ es igual al valor de la respuesta observada $y_i$, i.e. el modelo saturado genera un ajuste perfecto.

-   El modelo m谩s elaborado proporciona el mejor ajuste, por lo tanto debe tener la log-verosimilitud m谩s alta (que cualquier otro GLM ajustado)

### Devianza

-   La devianza es una medida de bondad de ajuste que se basa en la verosimilitud.

    -   Mide cu谩nto se desv铆a el GLM ajustado, en t茅rminos de la log-verosimilitud, del GLM "m谩s elaborado", que se conoce como el [modelo saturado]{.ul}.

**Definici贸n.**

$$
D = 2(l_{\text{SAT}} - \underbrace{l}_{\text{modelo que} \\ \text{estoy evaluando}})
$$

-   Una devianza grande es indicativo de un ajuste pobre.

-   Una devianza peque帽a es indicativo de buen ajuste.

### 驴C贸mo se obtiene la devianza?

-   **Paso 1:** Escribir una expresi贸n gen茅rica para la funci贸n de verosimilitud, en t茅rminos de las medias desconocidas $\mu_1, ..., \mu_n$ (Recu茅rdese que se est谩 inclinado a tratar a la media como un par谩metro GLM).

-   **Paso 2:** Hacer las sustituciones

    -   $\mu_i \mapsto y_i$ para el modelo saturado. Con esto tenemos $l_{\text{SAT}}$

    -   $\mu_i \mapsto \hat{\mu}_i$ para el modelo saturado. Con esto tenemos $l$

-   **Paso 3:** Hacer el c谩lculo $D = 2(l_{SAT} - l)$ en t茅rminos de $y_i'^s$ y $\hat{\mu}_i'^s$

**Ejemplo (Regresi贸n Poisson)**

Para el caso de que $y_1, ... , y_n$ tenga distribuci贸n Poisson con medias $\mu_1, ..., \mu_n$ respectivamente:

-   **Paso 1:** Expresi贸n gen茅rica: \begin{align*}
    l(\mu_1, ..., \mu_n) 
    &= \log\bigg(\prod_{i=1} ^n f(y_i)\bigg) \\
    & = \log\bigg(\prod_{i=1} ^n \frac{e^{-\mu_i}\mu_i^{y_i}}{y_i!}\bigg) \\
    &= \sum_{i=1}^n \bigg(-\mu_i +y_i\log(\mu_i) - \log(y_i!)\bigg)
    \end{align*}

-   **Paso 2:** Hacer las sustituciones

    -   $\mu_i \mapsto y_i$ para el modelo saturado.

        $$
        l_{SAT} = \sum_{i=1}^n \bigg(-y_i +y_i\log(y_i) - \log(y_i!)\bigg)
        $$

    -   $\mu_i \mapsto \hat{\mu}_i$ para el modelo saturado.

        $$
        l = \sum_{i=1}^n \bigg(-\hat{\mu}_i +y_i\log(\hat{\mu}_i) - \log(y_i!)\bigg)
        $$

-   **Paso 3:**

    $$
    \begin{align*}
    D &= 2(l_{SAT} -l) = 2\sum_{i=1}^n \bigg( \hat{\mu}_i - y_i + y_i(\log(y_i) - \log(\hat{\mu}_i)) \bigg) \\
    &= 2\sum_{i=1}^n \bigg( y_i\log(\frac{y_i}{\hat{\mu}_i})  - (y_i -  \hat{\mu}_i) \bigg) 
    \end{align*}
    $$

-   $D$ debe ser peque帽o. Intuitivamente "Si tengo 2 modelos, me quedo con el de devianza menor"

### pseudo$R^2$

**Definici贸n.**

$$
pseudoR^2 = \frac{l - l_{IID}}{l_{SAT} - l_{IID}}
$$

pseudo$R^2 \in (0,1)$

pseudo$R^2$ grande es indicativo de mejor ajuste del modelo.

Donde:

-   $l_{\text{SAT}}$: log-verosimilitud del modelo saturado.

-   $l$: log-verosimilitud del GLM en consideraci贸n.

-   $l_{\text{IID}}$: log-verosimilitud cl谩sica que se aprende en Inferencia Estad铆stica.

### AIC y BIC

Se tiene inter茅s en pruebas de hip贸tesis y 2 criteriors de selecci贸n de modelos

AIC = $-2l + 2p$

BIC = $-2l + p\log(n)$
